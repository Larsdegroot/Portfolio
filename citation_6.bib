@article{Ye2016,
   abstract = {<p> <bold>Motivation.</bold> The third generation sequencing (3GS) technology generates long sequences of thousands of bases. However, its current error rates are estimated in the range of 15–40%, significantly higher than those of the prevalent next generation sequencing (NGS) technologies (less than 1%). Fundamental bioinformatics tasks such as <italic>de novo</italic> genome assembly and variant calling require high-quality sequences that need to be extracted from these long but erroneous 3GS sequences. </p>},
   author = {Chengxi Ye and Zhanshan (Sam) Ma},
   doi = {10.7717/peerj.2016},
   issn = {2167-8359},
   journal = {PeerJ},
   month = {6},
   pages = {e2016},
   title = {Sparc: a sparsity-based consensus algorithm for long erroneous sequencing reads},
   volume = {4},
   url = {https://peerj.com/articles/2016},
   year = {2016},
}
@generic{Liu2012,
   abstract = {With fast development and wide applications of next-generation sequencing (NGS) technologies, genomic sequence information is within reach to aid the achievement of goals to decode life mysteries, make better crops, detect pathogens, and improve life qualities. NGS systems are typically represented by SOLiD/Ion Torrent PGM from Life Sciences, Genome Analyzer/HiSeq 2000/MiSeq from Illumina, and GS FLX Titanium/GS Junior from Roche. Beijing Genomics Institute (BGI), which possesses the worlds biggest sequencing capacity, has multiple NGS systems including 137 HiSeq 2000, 27 SOLiD, one Ion Torrent PGM, one MiSeq, and one 454 sequencer. We have accumulated extensive experience in sample handling, sequencing, and bioinformatics analysis. In this paper, technologies of these systems are reviewed, and first-hand data from extensive experience is summarized and analyzed to discuss the advantages and specifics associated with each sequencing system. At last, applications of NGS are summarized. © Copyright 2012 Lin Liu et al.},
   author = {Lin Liu and Yinhu Li and Siliang Li and Ni Hu and Yimin He and Ray Pong and Danni Lin and Lihua Lu and Maggie Law},
   doi = {10.1155/2012/251364},
   issn = {11107243},
   journal = {Journal of Biomedicine and Biotechnology},
   pmid = {22829749},
   title = {Comparison of next-generation sequencing systems},
   volume = {2012},
   year = {2012},
}
@article{Rausch2009,
   abstract = {Motivation: Novel high-throughput sequencing technologies pose new algorithmic challenges in handling massive amounts of short-read, high-coverage data. A robust and versatile consensus tool is of particular interest for such data since a sound multi-read alignment is a prerequisite for variation analyses, accurate genome assemblies and insert sequencing. Results: A multi-read alignment algorithm for de novo or reference-guided genome assembly is presented. The program identifies segments shared by multiple reads and then aligns these segments using a consistency-enhanced alignment graph. On real de novo sequencing data obtained from the newly established NCBI Short Read Archive, the program performs similarly in quality to other comparable programs. On more challenging simulated datasets for insert sequencing and variation analyses, our program outperforms the other tools. © The Author 2009. Published by Oxford University Press. All rights reserved.},
   author = {Tobias Rausch and Sergey Koren and Gennady Denisov and David Weese and Anne Katrin Emde and Andreas Döring and Knut Reinert},
   doi = {10.1093/bioinformatics/btp131},
   issn = {13674803},
   issue = {9},
   journal = {Bioinformatics},
   pages = {1118-1124},
   pmid = {19269990},
   title = {A consistency-based consensus algorithm for de novo and reference-guided sequence assembly of short reads},
   volume = {25},
   year = {2009},
}
@article{Wick2019,
   abstract = {Background: Basecalling, the computational process of translating raw electrical signal to nucleotide sequence, is of critical importance to the sequencing platforms produced by Oxford Nanopore Technologies (ONT). Here, we examine the performance of different basecalling tools, looking at accuracy at the level of bases within individual reads and at majority-rule consensus basecalls in an assembly. We also investigate some additional aspects of basecalling: training using a taxon-specific dataset, using a larger neural network model and improving consensus basecalls in an assembly by additional signal-level analysis with Nanopolish. Results: Training basecallers on taxon-specific data results in a significant boost in consensus accuracy, mostly due to the reduction of errors in methylation motifs. A larger neural network is able to improve both read and consensus accuracy, but at a cost to speed. Improving consensus sequences ('polishing') with Nanopolish somewhat negates the accuracy differences in basecallers, but pre-polish accuracy does have an effect on post-polish accuracy. Conclusions: Basecalling accuracy has seen significant improvements over the last 2 years. The current version of ONT's Guppy basecaller performs well overall, with good accuracy and fast performance. If higher accuracy is required, users should consider producing a custom model using a larger neural network and/or training data from the same species.},
   author = {Ryan R. Wick and Louise M. Judd and Kathryn E. Holt},
   doi = {10.1186/s13059-019-1727-y},
   issn = {1474760X},
   issue = {1},
   journal = {Genome Biology},
   keywords = {Basecalling,Long-read sequencing,Oxford Nanopore},
   month = {6},
   pmid = {31234903},
   publisher = {BioMed Central Ltd.},
   title = {Performance of neural network basecalling tools for Oxford Nanopore sequencing},
   volume = {20},
   year = {2019},
}
@article{Sohn2018,
   abstract = {As the advent of next-generation sequencing (NGS) technology, various de novo assembly algorithms based on the de Bruijn graph have been developed to construct chromosome-level sequences. However, numerous technical or computational challenges in de novo assembly still remain, although many bright ideas and heuristics have been suggested to tackle the challenges in both experimental and computational settings. In this review, we categorize de novo assemblers on the basis of the type of de Bruijn graphs (Hamiltonian and Eulerian) and discuss the challenges of de novo assembly for short NGS reads regarding computational complexity and assembly ambiguity. Then, we discuss how the limitations of the short reads can be overcome by using a single-molecule sequencing platform that generates long reads of up to several kilobases. In fact, the long read assembly has caused a paradigm shift in whole-genome assembly in terms of algorithms and supporting steps. We also summarize (i) hybrid assemblies using both short and long reads and (ii) overlap-based assemblies for long reads and discuss their challenges and future prospects. This review provides guidelines to determine the optimal approach for a given input data type, computational budget or genome.},
   author = {Jang Il Sohn and Jin Wu Nam},
   doi = {10.1093/bib/bbw096},
   issn = {14774054},
   issue = {1},
   journal = {Briefings in Bioinformatics},
   keywords = {De Bruijn graph,De novo assembly algorithms,Next-generation sequencing,Single-molecule sequencing},
   pages = {23-40},
   pmid = {27742661},
   publisher = {Oxford University Press},
   title = {The present and future of de novo whole-genome assembly},
   volume = {19},
   year = {2018},
}
