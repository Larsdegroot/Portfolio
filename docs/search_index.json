[["index.html", "Portfolio 1 Welcome", " Portfolio Lars de Groot 1 Welcome Hey Im Lars! I Enjoy hiking in the Apls, collecting and growing rare plants and whenever i get the chance i like to build something may it be wooden furniture, a greenhouse cabinet or a program. I am currently a student studying life sciences at the Hogenschool Utrecht, and as of late im trying to steer my education to become an bioinformatician. ive started with learning and using R, and im planning to get into Python. This bookdown serves as an portfolio, displaying everything ive learned so far. For any future employer or internship co√∂rdinator my CV can be found here. "],["c.-elegans-plate-experiment.html", "2 C. elegans plate experiment", " 2 C. elegans plate experiment The data used in this report comes from a experiment where C. elegans was exposed to varying concentrations of three different compounds: 2,6-diisopropylnaphthalene, decane and naphthalene. After an incubation period the number of offspring where counted. library(tidyverse) # 1.3.1 library(readxl) #1.3.1 library(here) #1.0.1 library(drc) #3.0-1 library(knitr) #1.33 library(ggpubr) # 0.4.0 library(captioner) #2.2.3.900 library(kableExtra) #1.3.4 #first excel sheet is read ce_data &lt;- readxl::read_xlsx(here(&quot;data/CE.LIQ.FLOW.062_Tidydata.xlsx&quot;)) names(ce_data) ## [1] &quot;plateRow&quot; &quot;plateColumn&quot; &quot;vialNr&quot; &quot;dropCode&quot; &quot;expType&quot; ## [6] &quot;expReplicate&quot; &quot;expName&quot; &quot;expDate&quot; &quot;expResearcher&quot; &quot;expTime&quot; ## [11] &quot;expUnit&quot; &quot;expVolumeCounted&quot; &quot;RawData&quot; &quot;compCASRN&quot; &quot;compName&quot; ## [16] &quot;compConcentration&quot; &quot;compUnit&quot; &quot;compDelivery&quot; &quot;compVehicle&quot; &quot;elegansStrain&quot; ## [21] &quot;elegansInput&quot; &quot;bacterialStrain&quot; &quot;bacterialTreatment&quot; &quot;bacterialOD600&quot; &quot;bacterialConcX&quot; ## [26] &quot;bacterialVolume&quot; &quot;bacterialVolUnit&quot; &quot;incubationVial&quot; &quot;incubationVolume&quot; &quot;incubationUnit&quot; ## [31] &quot;incubationMethod&quot; &quot;incubationRPM&quot; &quot;bubble&quot; &quot;incubateTemperature&quot; The Excel file contains a lot of data not necessary for this report. The variable used in this report are: expType: Experiment type RawData: Number of offspring compName: Compound name compConcentration: Compound concentration compUnit: Unit of measurement #getting rid of unnecessary columns ce_data &lt;- dplyr::select(ce_data, c(&quot;expType&quot;, &quot;RawData&quot;, &quot;compName&quot;, &quot;compConcentration&quot;, &quot;compUnit&quot;)) Table 1: the first 10 rows of the trimmed C. elegans data expType RawData compName compConcentration compUnit experiment 44 2,6-diisopropylnaphthalene 4.99 nM experiment 37 2,6-diisopropylnaphthalene 4.99 nM experiment 45 2,6-diisopropylnaphthalene 4.99 nM experiment 47 2,6-diisopropylnaphthalene 4.99 nM experiment 41 2,6-diisopropylnaphthalene 4.99 nM experiment 35 2,6-diisopropylnaphthalene 4.99 nM experiment 41 2,6-diisopropylnaphthalene 4.99 nM experiment 36 2,6-diisopropylnaphthalene 4.99 nM experiment 40 2,6-diisopropylnaphthalene 4.99 nM experiment 38 2,6-diisopropylnaphthalene 4.99 nM #Certain columns where missclassed by read_xlsx(), correcting: ce_data$compConcentration &lt;- as.numeric(ce_data$compConcentration) ce_data$compName &lt;- as_factor(ce_data$compName) The compound were tested at different concentration gradients per compound. A decrease in offspring count can be seen with the increase of each compounds concentration iso &lt;- ce_data %&gt;% dplyr::filter(compName == &quot;2,6-diisopropylnaphthalene&quot;) %&gt;% ggplot2::ggplot(aes(x = compConcentration, y = RawData)) + ggplot2::geom_point() + ggplot2::geom_smooth(method = &quot;lm&quot;) + ggplot2::labs(x = &quot;Compand concentration in nM&quot;, y = &quot;Number of offspring&quot;, title = &quot;2,6-diisopropylnaphthalene&quot;) decane &lt;- ce_data %&gt;% dplyr::filter(compName == &quot;decane&quot;) %&gt;% ggplot2::ggplot(aes(x = compConcentration, y = RawData)) + ggplot2::geom_point() + ggplot2::geom_smooth(method = &quot;lm&quot;) + ggplot2::labs(x = &quot;Compand concentration in nM&quot;, y = &quot;Number of offspring&quot;, title = &quot;Decane&quot;) naph &lt;- ce_data %&gt;% dplyr::filter(compName == &quot;naphthalene&quot;) %&gt;% ggplot2::ggplot(aes(x = compConcentration, y = RawData)) + ggplot2::geom_point() + ggplot2::geom_smooth(method = &quot;lm&quot;) + ggplot2::labs(x = &quot;Compand concentration in nM&quot;, y = &quot;Number of offspring&quot;, title = &quot;Naphthalene&quot;) ggarrange(iso, decane, naph) Figure 1: The effect of the compounds on the number of C. elegans offspring. However the experiment was designed with the creation of a dose-response curve in mind. So the compound concentration is on a logarithmic scale. #removed geom_smooth linear method iso &lt;- ce_data %&gt;% dplyr::filter(compName == &quot;2,6-diisopropylnaphthalene&quot; | compName == &quot;S-medium&quot;) %&gt;% ggplot2::ggplot(aes(x = compConcentration, y = RawData)) + ggplot2::geom_jitter(width = 0.1) + ggplot2::geom_smooth() + ggplot2::labs(x = &quot;Compand concentration in nM&quot;, y = &quot;Number of offspring&quot;, title = &quot;2,6-diisopropylnaphthalene&quot;) decane &lt;- ce_data %&gt;% dplyr::filter(compName == &quot;decane&quot; | compName == &quot;S-medium&quot;) %&gt;% ggplot2::ggplot(aes(x = compConcentration, y = RawData)) + ggplot2::geom_jitter(width = 0.1) + ggplot2::geom_smooth() + ggplot2::labs(x = &quot;Compand concentration in nM&quot;, y = &quot;Number of offspring&quot;, title = &quot;Decane&quot;) naph &lt;- ce_data %&gt;% dplyr::filter(compName == &quot;naphthalene&quot; | compName == &quot;S-medium&quot;) %&gt;% ggplot2::ggplot(aes(x = compConcentration, y = RawData)) + ggplot2::geom_jitter(width = 0.1) + ggplot2::geom_smooth() + ggplot2::labs(x = &quot;Compand concentration in nM&quot;, y = &quot;Number of offspring&quot;, title = &quot;Naphthalene&quot;) iso + ggplot2::scale_x_log10() Figure 2: The effect of 2,6-diisopropylnaphthalene on the number of offspring of C. elegans. The X-axis is logmarithmicly scaled decane + ggplot2::scale_x_log10() Figure 3: The effect of decane on the number of offspring of C. elegans. The X-axis is logmarithmicly scaled naph + ggplot2::scale_x_log10() Figure 4: The effect of naphthalene on the number of offspring of C. elegans. The X-axis is logmarithmicly scaled Figure 2, Figure 3 and Figure 4 are supposed to resemble dose response curves, however geom_smooth method of applying a curve to the data points isnt meant for creating a dose response curve. A package which is meant for this is {drc}. The model to be fitted to the response data will be a four parameter log-logistic function, abbreviated to LL.4 in {drc}. this means that 1 out of the five parameters are pre-set, which are f = 1. Normally LL.3 would be used where c is set to 0, because the response data is discrete (number of offspring). However this doesnt produce a good fit, because there are no datapoints where the number of offspring is actually zero. knitr::include_graphics(here::here(&quot;images/LL4.jpg&quot;)) Figure 5: Formula for LL.4. f = model function, b = steepness of curve, c = lower limit of the response, d = upper limit of the response and e = ED50. (Ritz et al. 2015, 4) #fitting a dose response model on the data drc_iso &lt;- drc::drm(RawData ~ compConcentration, data = dplyr::filter(ce_data, compName == &quot;2,6-diisopropylnaphthalene&quot; | compName == &quot;S-medium&quot;), fct = LL.4()) drc_decane &lt;- drc::drm(RawData ~ compConcentration, data = dplyr::filter(ce_data, compName == &quot;decane&quot;| compName == &quot;S-medium&quot;), fct = LL.4()) drc_naph &lt;- drc::drm(RawData ~ compConcentration, data = dplyr::filter(ce_data, compName == &quot;naphthalene&quot;| compName == &quot;S-medium&quot;), fct = LL.4()) plot(drc_iso, type = &quot;all&quot;, xlab = &quot;2,6-diisopropylnaphthalene concentration in nM&quot;, ylab = &quot;number of offspring&quot;) Figure 6: The dose response curve of 2,6-diisopropylnaphthalene on the number of offspring of C. elegans. A decrease in number of offspring in C. elegans after incubation with 2,6-diisopropylnaphthalene is seen. However the curve is not steep. plot(drc_decane, type = &quot;all&quot;, xlab = &quot;decane concentration in nM&quot;, ylab = &quot;number of offspring&quot;) Figure 7: The dose response curve of decane on the number of offspring of C. elegans. Decane has a negative effect on the number of offspring of C. elegans which seems to start around a decane concentration of 4.99e-02. The curve is also very steep. plot(drc_naph, type = &quot;all&quot;, xlab = &quot;naphthalene concentration in nM&quot;, ylab = &quot;number of offspring&quot;) Figure 8: The dose response curve of naphthalene on the number of offspring of C. elegans. Decane has a negative effect on the number of offspring of C. elegans which seems to start around a naphthalene concentration of 4.99e-01. The curve is also very steep. Refrences "],["an-exercise-in-open-science.html", "3 An exercise in Open science 3.1 Score: 3.2 Explanation:", " 3 An exercise in Open science This exercise is about identifying reproducibility issues in a scientific publication. I use the criteria for reproduciblity that is publically available here. Article used: A retrospective cluster analysis of COVID-19 cases by county 3.1 Score: Table 1: scoring of A retrospective cluster analysis of COVID-19 cases by county for the criteria laid out here. Transparency.Criteria Definition Score Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. TRUE Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a studys data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. TRUE Data Location Where the articles data can be accessed, either raw or processed. TRUE Study Location Author has stated in the methods section where the study took place or the datas country/region of origin. TRUE Author Review The professionalism of the contact information that the author has provided in the manuscript. TRUE Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. FALSE Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. FALSE Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. TRUE 3.2 Explanation: At the top of the article the authors names are displayed, with the names of the universities the study at or are part of. Of one of the authors the email is given. This information falls under the author review criteria. this information is sufficient for the goal of open science in this case because email, department and phone number information can be found online for these scientist. however this might not always be the case so the availability of the email address of the paper is appreciated. The article aims to better understand the pattern of outbreaks of COVID-19 within the US. it states this as a study purpose at the end of the introduction paragraph. it then continuous to formulate 3 research questions which further clarifies the study purpose: How many distinct clusters of counties exhibit similar COVID-19 patterns in the time-series of daily confirmed cases? What is the geographic distribution of the counties within each cluster? Are county-level demographic, socioeconomic and political variables associated with the COVID-19 case patterns? the article then starts to give a detailed description of the methods used in the study, it does this in stages. In the paragraph Stage 0: Data Acquisition and Preprocessing, it starts with explaining what the sources are for the data used in the study. this is a example of the criteria data location. However no link was supplied. At further inspection the data used here came from the R package 'COVID-19', And in the references it does specify the date of accessing the data hub. Also the locations of supplementary data that wasnt taken from other sources is listed in the same paragraph. However not all of the links provided work anymore. Table 2: the provided links in paper. many dont work anymore showing that simply providing links isnt always enough because of the changing nature of the internet. Data used Link Provided Link working? Rural and Underserved Counties List https://www.consumerfinance.gov/documents/8911/cfpbrural-underserved-list2020.csv FALSE Population Density https://www2.census.gov/library/publications/2011/compendia/usacounties/excel/LND01.xls FALSE Voting Results https://doi.org/10.7910/DVN/VOQCHQ TRUE Governors Party Affiliation https://en.wikipedia.org/w/index.php?title=ListofUnitedStatesgovernors&amp;oldid=977828843 TRUE Health Variables https://khn.org/news/as-coronavirus-spreadswidely-millionsof-older-americans-live-in-counties-with-no-icu-beds/#lookup FALSE Region https://www.cdc.gov/coordinatedchronic/docs/nccdphp-regions-map.pdf FALSE This shows how simply providing links to data used isnt enough because the validity of the repository which host the data isnt always guaranteed. but later in the article under Supporting information it also links to a github page. This is a great example of open science because the data location where the data originates from is stated, and a neat collection of data used within the study is given. within this same paragraph it is stated: To capture the progression of disease in the U.S., the number of confirmed COVID-19 cases at the county level from March 1, 2020 through October 24, 2020 was extracted from the COVID-19 data hub [4]. Only data from the contiguous 48 states were included. In this sentence the region of origin of the data is described, thus the criteria Study location has been met. Under acknowledgments is where you would expect there to a funding statement. But it simple states none. hereby it is neither denied or confirmed that this study is funded so this article does not meet the criteria Funding statement. After the references there is supporting information paragraph, which states what program was used (R), the version (4.0.2), and gives a link to a github page, which sadly no longer works. But the user page can be derivated from the link and so the data used (raw and processed) can be accessed. which is why the article meets the criteria Data Availability Statement and Code Availability. Further more no ethical concerns were raised in the article so it doesnt meet the criteria Ethics Statement. However considering the way the study was conducted, there was no need for a ethics statement. "],["an-exercise-in-reproducibility.html", "4 An exercise in reproducibility", " 4 An exercise in reproducibility Project selected: Bats and COVID-19 the code in this project downloads gtrends data for the query bats and the word bat translated in different languages in there associated geolocations, for the time period from 2016 to 2020. using this it plots figures that outline the correlation between searches about covid-19 and searches about bats. The readability of the code of this project is well done. Chunks of code have comments explaining the general function of the chunk. This makes it easy to understand the general make up of the script. i would only have to try to understand the packages that im not familiar with to fully understand what the purpose is of the code. To test the reproducibility of the script i will try to reproduce one of the figures from the project. library(tidyverse) #1.3.1 library(readxl) #1.3.1 tv.dat &lt;- read.csv(file=&quot;~/Documents/Research_Projects/Ongoing/BatsCovid/NewAnalysis_Dec2020/Data/Weekly/USTelevision/GDELTBatsUS1620.csv&quot;) tv.dat$date &lt;- ymd(tv.dat$date) tv.dat[61, &quot;date&quot;] &lt;- &quot;2021-01-01&quot; tv.dat[61, &quot;value&quot;] &lt;- NA tv.dat[61, &quot;X&quot;] &lt;- 61 ggplot() + geom_line(data=tv.dat, aes(x=X, y=value), size=0.8, color=&quot;steelblue&quot;, linetype=&quot;solid&quot;) + theme_bw() + scale_x_continuous(breaks=c(1, 13, 25, 37, 49, 61), labels=c(&quot;2016&quot;, &quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;)) + theme(axis.text.x = element_text(colour=&quot;black&quot;,size=14,angle=0,hjust=.5,vjust=.5,face=&quot;plain&quot;), axis.text.y = element_text(colour=&quot;black&quot;,size=14,angle=0,hjust=1,vjust=0,face=&quot;plain&quot;), axis.title.x = element_text(colour=&quot;black&quot;,size=12,angle=0,hjust=.5,vjust=.5,face=&quot;plain&quot;), axis.title.y = element_text(colour=&quot;black&quot;,size=12,angle=90,hjust=.5,vjust=.5,face=&quot;plain&quot;), plot.title = element_text(size=22, face=&quot;bold&quot;)) + theme(strip.background =element_rect(fill=&quot;wheat&quot;)) + labs(x=&quot;&quot;, y=&quot;&quot;) + theme(strip.text = element_text(colour = &quot;black&quot;, size=14, face=&quot;bold&quot;)) Running this produces two errors: Error in file(file, rt) : cannot open the connection Error in ymd(tv.dat$date) : could not find function ymd. these are easy to fix. for the first i changed the filepath so it matches mine. and installed the package {lubridate}, ymd() is part of the {lubridate} package. library(lubridate) tv.dat &lt;- read.csv(file=(here(&quot;data/bats_and_covid19/GDELTBatsUS1620.csv&quot;))) tv.dat$date &lt;- ymd(tv.dat$date) tv.dat[61, &quot;date&quot;] &lt;- &quot;2021-01-01&quot; tv.dat[61, &quot;value&quot;] &lt;- NA tv.dat[61, &quot;X&quot;] &lt;- 61 ggplot() + geom_line(data=tv.dat, aes(x=X, y=value), size=0.8, color=&quot;steelblue&quot;, linetype=&quot;solid&quot;) + theme_bw() + scale_x_continuous(breaks=c(1, 13, 25, 37, 49, 61), labels=c(&quot;2016&quot;, &quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;)) + theme(axis.text.x = element_text(colour=&quot;black&quot;,size=14,angle=0,hjust=.5,vjust=.5,face=&quot;plain&quot;), axis.text.y = element_text(colour=&quot;black&quot;,size=14,angle=0,hjust=1,vjust=0,face=&quot;plain&quot;), axis.title.x = element_text(colour=&quot;black&quot;,size=12,angle=0,hjust=.5,vjust=.5,face=&quot;plain&quot;), axis.title.y = element_text(colour=&quot;black&quot;,size=12,angle=90,hjust=.5,vjust=.5,face=&quot;plain&quot;), plot.title = element_text(size=22, face=&quot;bold&quot;)) + theme(strip.background =element_rect(fill=&quot;wheat&quot;)) + labs(x=&quot;&quot;, y=&quot;&quot;) + theme(strip.text = element_text(colour = &quot;black&quot;, size=14, face=&quot;bold&quot;)) Figure 1: This graph shows the increase of news about bats on american television. "],["good-workflow-practices-guerrilla-analytics.html", "5 Good Workflow practices: Guerrilla analytics", " 5 Good Workflow practices: Guerrilla analytics Within all work there are good practices and rules to follow. Which can also be called workflows. Datascience is no different, and a book written about the matter is Guerrilla Analytics. in here Enda Ridge talks about 7 principles to uphold while working on a datascience project. These principle help keep the project understandable, explainable and reproducible. 5.0.0.1 the Guerrila analytics: Space is cheap, confusion is expensive Prefer simple, visual project structures and conventions Prefer automation with program code Maintain a link between data on the file system, data in the analytics environment, and data in work products Version control changes to data and analytics code Consolidate team knowledge in version-controlled builds Prefer analytics code that runs from start to finish To help me uphold the first 3 principles i automated the creation of a new project folder with a simple R script: #create a new project in current directory library(usethis) #2.0.1 currect_dir &lt;- getwd() go &lt;- readline(prompt = paste(&quot;Do you want to creat a new project folder at&quot;, currect_dir, &quot;? y/n: &quot;, sep = &quot; &quot;)) if (go == &quot;y&quot;){ #make project folder project.name &lt;- readline(prompt = &quot;Enter project name: &quot;) dir.create(project.name) for (i in c(&quot;data&quot;, &quot;doc&quot;, &quot;external&quot;, &quot;output&quot;, &quot;src&quot;)){ dir.create(paste0(project.name, &quot;/&quot;, i)) } #make a project usethis::create_project(project.name) # #connect to git # usethis::use_git() # usethis::use_github() } else { stop()} This script creates a standard file structure. The .txt files are there to explain the general purpose of the directory. require(fs) dir_tree(&quot;/Users/larsd/Documents/R/Project/&quot;, recurse = TRUE) ## /Users/larsd/Documents/R/Project/ ## +-- data ## | \\-- Raw data files.txt ## +-- doc ## | \\-- Documentation.txt ## +-- external ## | \\-- Any external or miscellaneous files .txt ## +-- output ## | \\-- If output needs to be saved it ends up here.txt ## +-- RMD files in the main project folder.Rmd ## \\-- src ## \\-- Script and or collections of function used in the analysis.txt This structure will also be applied to the project folder that contains the files for this bookdown portfolio. The directory tree seen below is simplified so that its understandable. The true directory tree is much to crowded. dir_tree(recurse = 1) ## . ## +-- data ## | +-- 2021-01-13_Variable_Dictionary_and_Disclaimer_hosp_icu_all_data.pdf ## | +-- 2021-01-13_Variable_Dictionary_and_Disclaimer_weekly_testing_data_EUEEAUK.pdf ## | +-- A retrospective cluster analysis of COVID-19 cases by county.pdf ## | +-- bats_and_covid19 ## | +-- CE.LIQ.FLOW.062_Tidydata.xlsx ## | +-- custom.geo.json ## | +-- dengue_data.txt ## | +-- Description-and-disclaimer_daily_reporting.pdf ## | +-- ECDC_hospital.csv ## | +-- ECDC_testing.csv ## | +-- EVDV_vaccine.csv ## | +-- flu_data.txt ## | +-- rosalind_cons.txt ## | +-- rosalind_cons_single.txt ## | +-- rosalind_dna.txt ## | +-- rosalind_gc.txt ## | +-- rosalind_grph.txt ## | +-- rosalind_grph_single.txt ## | +-- rosalind_hamm.txt ## | +-- rosalind_iev.txt ## | +-- rosalind_iprb.txt ## | +-- rosalind_prot.txt ## | +-- rosalind_revc.txt ## | +-- rosalind_rna.txt ## | +-- rosalind_subs.txt ## | +-- single.txt ## | \\-- Variable_Dictionary_VaccineTracker-03-2021.pdf ## +-- docs ## +-- ext ## | +-- 41572_2018_Article_2.pdf ## | +-- denguedhf-information-for-health-care-practitioners_2009.pdf ## | \\-- export.bib ## +-- images ## | +-- hike_italy.jpg ## | +-- LL4.jpg ## | \\-- workflowsdb_in_Dbeaver.JPG ## +-- index.rmd ## +-- output ## | +-- dengue.csv ## | +-- dengue.rds ## | +-- ECDC_covid.csv ## | +-- flu.csv ## | +-- flu.rds ## | +-- gapminder.csv ## | \\-- gapminder.rds ## +-- Portfolio.Rproj ## +-- portfolio1.1_DSFB.rmd ## +-- portfolio1.2p1_DSFB.rmd ## +-- portfolio1.2p2_DSFB.rmd ## +-- portfolio2_DSFB.rmd ## +-- portfolio6.2_DSFB.rmd ## +-- portfolio7_DSFB.rmd ## +-- portfolio8_DSFB.rmd ## +-- portfolio9_DSFB.rmd ## +-- python ## | +-- Lib ## | +-- pyvenv.cfg ## | \\-- Scripts ## +-- src ## | +-- create_project.R ## | \\-- port_src7.R ## +-- _main.Rmd ## \\-- _main_files ## \\-- figure-html "],["learning-python.html", "6 Learning Python 6.1 Rosalind exercises:", " 6 Learning Python Im aspiring to become an bioinformatician. This is because ive found the act of coding, and thinking how a program should fit together scratches an itch to create and design for me. Which is a need that i cant fulfill when doing wet lab work or writing lab journal reports. Bioinformatics as a field is quite diverse in a way that what one bioinformatician does, could be different from what an other bioinformatician does. There are lots of different software tools you need to learn depending on which subject within biology you are working with. However i have found that most bioinformaticians use a combination of R and Python for most of their work. Ive made a good start on R during my minor, but i dont have any experience with Python. To start to learn Python im planning on going through the book Think Python by Allen B. Downey. Its very important to gain experience when learning a new programming language, and Think Python has exercises to play with. However these are not focused on bioinformatics. To gain experience with python for bioinformatics i want to go over Rosalind exercises. Rosalind is a website that has various coding problems that you could encounter as an bioinformatician, so its a great way to apply what ive learned in python to bioinformatic problems. A Internship that ive applied for also recommended me three Coursera courses that i am planning to delve into: Python for Everybody Specialization An Introduction to Interactive Programming in Python (Part 1) An Introduction to Interactive Programming in Python (Part 2) 6.1 Rosalind exercises: 6.1.1 Q1 Counting DNA nucleotides def count(s, c): count = 0 for i in s: if c == i: count += 1 return count with open(&quot;data/rosalind_dna.txt&quot;, &#39;r&#39;)as seq: seq_1 = seq.readline() out = [] for i in [&#39;A&#39;, &#39;C&#39;, &#39;G&#39;, &#39;T&#39;]: out.append(count(seq_1, i)) print(out) ## [213, 241, 215, 243] 6.1.2 Q2 Transcribing DNA into RNA with open(&#39;data/rosalind_rna.txt&#39;, &#39;r&#39;)as rna: s = rna.readline().rstrip() u = [] for i in list(s): if i == &#39;T&#39;: u.append(&#39;U&#39;) else: u.append(i) &#39;&#39;.join(u) ## &#39;GCUGUGACCUCUCGCACGAUCUGUGAGCGUGGCUUGGGAAUCACGCGCUGUCUCUUCACUCCGAUCGCCUGACAGCUGAAGGGUGCUGAAGCUGGUUAACCAGACGAGGUGACCUGGAGAAUACUAUUUUGAGAAGUGGAUAGCAGAUGAGAAGUAGCUAGGACCUAAUUACGGUUCACCAUCGAGCCGCUAGGAAAUAGAUAAGAGCCUUGCCACAAGUGGGUGACGAUAAUACCCAGUGCCCCCCACAUGAGUAGUCGAUCGAGGGCUCGAUCCACUCCUGGACCCGGGGCCAAAUGGGCAGUAUACCACGACAAGCUGUGAUCCGCCACUCUGGAGGUUCUCACAUAACCUUUAGGUGUCGCCCUAUCUGAACAAGGUACUAGCGCUGCCUGGUACCGCCCGUUGCGGAGGUUAGGGCUUAUGCCACAUCAUACUUAGUGCGUUUCGUAAAAAGUGAAUAUGCGGCGCGCCAUAAUGUUGUAUUCGCUGCCUGUGUUCGGAGCGCGCCAACAGCACCUCAGCCUCCACAGCCAUAAGACAGGUACUCGAGGUUGAAACUAGUAGGGAAGAAGGCGCCUCAUUUUCAUAUCCUGCCCUCCAGUCAACAUGGGACAGCGACCAAGAAGAACACGUAUGGUGCCUAUGCCAGCCUCAUAGACCAAAGGGUGCAGAUAUGGUUACAACGCGGCGUCCGCGAACGACACCCGUUAAUCUUCCAUCGACAAUACUUUCUUCUGGCCGCGCAGGGGUGAUGGCAAAUCUGUAUUGGUCCUGAGGGAACGUUAACCAUCUCCGUACGACUGGCCAUCGUCCACUAUAGGGCCCCCGUCAGAGCUCAGGAAACUAAGGUCUCGCACUAAUCAGUGCUUCAUAGAGGCCGCCUUUGAUGUCUGCCGUUUUGUCCAGGGGGAGUCCCUGGUCAGUUCUUAUCGUCUACAGGUCAAACA&#39; 6.1.3 Q3 Complementing a Strand of DNA with open(&quot;data/rosalind_revc.txt&quot;, &#39;r&#39;) as dna: seq = dna.readline().rstrip() com = seq.translate(&#39;&#39;.maketrans(&#39;ATCG&#39;, &#39;TAGC&#39;)) #reverse string com[::-1] ## &#39;ATCAGTAGTAATTTAAAGATTGTGCTGTCCTAGCGTAGAGTATAGCGTGATATTGGATTATATTCCCTGATGCTCACGTCAGCGCCGGATAGTCGCTAACAGCACTCTAACATTGAAATCACTTGCCGCCCCAAGTAAACTCAGACATAACTACCCAGAGAAGCCCATATCTGAATCGTGACGTTAAACGCGCATTTCCCTGGATAGGTTGGAAAAAGTCCCCGTCGTGTCAGTCCTCATAGCTACGGCGTGCCAATACGTTATGCTTGAACGGGAACTTCTACTGTATTGGGGAACCTTCGGCGTACATCCAGAGAGCATGGCCGCCCGCTATAATGTACTTCCACGTGCAGCACCTATGGATTGGAACTCCTAGATACGTTATCCGGCCTGCTATGCACCATGCTGCCGTCTGCTCATGTAGTAACTTAACGAAACCGTATTCGAAGGGTATACACGTAAATTAAAGGGGCGGTCAAGTAGCGGAGCCACGTGCCGTCCACAAAACCATAATGGACGTTAGACTACTATGTGCGGTACGATCCACGCGTAGCGATTGCGGACCCGGTGTGGATTAGAACTGATAAATCGGAAGAAACTCTTCGTTATCCTCCAAGGATGTCAAGTAGGTGTCGTCACGGTAGACAACGTACCTTTTCGTGTCGATGACTTAATCAGAGCGTTAGCAACTTTTCTAAGGTTCCCGTTATATACTTGTCTTTGGGGTATTCTTAGCTCGCGATCATATCAGATGAATAGCCCGCGTCGACGGGGCTCTGTTCATACGGACTACTGCAGTCCATAGGCGCCACCCTTTCGCATGATCTTTCACTCTTAGATAGCTATACAGGATATCCTACCGAAATCTCCACGTAAGTCAATAAGCACTTTGAGACTATTTAGCTGTACACTGGGATTGTGGTACACAATGCATAGGTAATAAACCGAATAGCTCTTAGGAGCCCTGGTATATGTCTAATG&#39; 6.1.4 Q4 : Rabbits and Recurrence Relations def fibonacci(n, k): if n == 0: return 0 elif n == 1: return 1 else: return fibonacci(n-1, k) + fibonacci(n-2, k)*k fibonacci(31,3) ## 47079164257 6.1.5 Q5: Computing GC Content #convert multi to single line def to_sigle_fastq(in_file, out_file, header): &quot;&quot;&quot;&quot;&quot;&quot; with open(in_file, &#39;r&#39;)as fastq_multi, open(out_file, &#39;w&#39;) as fastq_single: block = [] for line in fastq_multi: if line.startswith(header): #if block contains anything, write block to file #this only writes the seq after a seq has been collected in block if block: fastq_single.write(&#39;&#39;.join(block) + &#39;\\n&#39;) block = [] #always write header line to file fastq_single.write(line) else: #add seq on other line to block block.append(line[:-1]) fastq_single.write(&#39;&#39;.join(block)) to_sigle_fastq(&#39;data/rosalind_gc.txt&#39;, &#39;data/single.txt&#39;, &#39;&gt;Rosalind&#39;) #put header and seq in seprate lists with equal length with open(&#39;data/single.txt&#39;, &#39;r&#39;)as fastq: id_seq = [] seq = [] for line in fastq: if &#39;&gt;Rosalind&#39; in line: id_seq.append(line[:-1]) else: seq.append(line[:-1]) print(id_seq, seq) #calculating cg content ## [&#39;&gt;Rosalind_6404&#39;, &#39;&gt;Rosalind_1930&#39;, &#39;&gt;Rosalind_8097&#39;, &#39;&gt;Rosalind_4648&#39;, &#39;&gt;Rosalind_0242&#39;, &#39;&gt;Rosalind_8179&#39;] [&#39;AATAATGGTGTAATCCCCTTACCTTATACGGGTGTCTAACCTCGATCCGAGGAGAAAGCAAAGAGGCTAGGACCAATGGCTCGCTAATGCTTAGGCTTGATCCGGAGGTCTGAACGGACGTGCTACGCGTGCCAGCGTGAGAACACCCTTCTTAATGCTATCACCGATTCGACCTAGATATGGGTCTGATCATTTGTCAGCACAGTCGGCAAGAATAATGCTTAGGGCCAGTTTTTTGGTTCAGCAGGCTAGAGGTTGCTACCCCAGTATACCCGCATATACTGGGATTTCCAAATAACAGTGCACGATTTGCGATCGAGAAGGTCCCAGGCTAGCAATTGGTTGTGGGTTCATCAGTGACCACGGCTTCGCTGGGAACTATTCGCAAAGAACAAGCCTAACACCAGCAAGGCCTTGAGACTGAAAGAATTCAGGGCTAGTAGCAGGGGCGACGGTGACTTTCCATTCGTTGAAAACTCCTCTCGTAGATCAGTACTGGCTGTGGCCTGTGCGGAAAACATGACGTCTTGTCGCACGCTTCAATCTGTGACGGTATCTGTACAGAGGCCCTAGTGTAGACCTAACGGGAACATCCTCGGACACCACCATCTTCAGAGGCTGAATCGTGCTTCAGTTAGCGTTTATGCTCCTTTGTGTTTGGAAATTATGAAAGTTCGTTATCGCCTTACATAATCCCTTTTTTGAGGGAGTGTTTAAAATGCAGCACGCGTCTTGATCGAGACCGAGTCATCCACGAAGAAGGTGAATAACTCCGTAGATTCTCGTGTGATACAACCTCCAGTGTCCCGCCTCCGCTGTGGGAGAGGAGAAAAATGCGCCTTGCGGCGCAGTGGGCCAGTGATCA&#39;, &#39;ATGGGAATCAATTATCTTTGATTTCGGTCCACCAAGGCCGATCGGGCCATGTTCATACCATATGACCCATTTATCTGGAACATGAAAACTTCGCTACTTTTCTCAAAATTTACCGGAAACCAGGGCGCCCCCGTTGCGGTCATGAGATCACTGGGATGAATGGAGTTCCAATTACGCTTACTGAAGCCTCATGGGGTAGACTAATGGTCTGGCGTACGAAGACATTATAAACGTTTCTCGTTCCGTTCTTAACTCCCTGAGGTGACGCCCCCACCCGAATGGGCGACTCAATATCCTATGCATGGGGCTCTTTCGCTCGTTACGAGTGTAATCAGCCAACTATGTTAATTTTAGAGGGGCCGACCTTCGTCCCTTCAAAAAGGAGTGTACACACATAAGATGGCATAGCCGTCTCAAGGTAATCACTGAAGCCCGCCGGATCGAACTAGCCGAAACCAGTGAAGGTCCTTGCGTCGTCCGTGTTGATCCCATCGAGCCCCGGATACTGTCAATAAATCCAATGCCGAAACTTATCAATAGTTCTCCGACTCTATGGATACAGGCAGTCTGAAGACAACGTCCAAGCGTCTATCTGCAAGAGATTAATCATGGTTACGTGGTGTTATATTAGGAGTGTCAAGGCTATTAATGGTGGTTGAACATAATTGCCCGGCTGGATACGTGAGTGGGGGGGCTCCTATCGTCTCGGGTATATAACCAGCGTTCAGGACCTCGTGATTTTTGGTGACGCCATTTCTGATATATCCTGAAGCTTCTGAGAATCCGCCAGATAGGATAATGGTGCCGGGACCGGCTCGCCGGGATGAGGTTCAGTCCTGCTATCTTTATAGCGATTTGTATTTAGTCTACTACTATAACATTACTTAAGGGTCGCTGCAGCACGGAACTGCACCCGCAGCCATCCACAGGAACTTAAGGGCGGTAATTGCCTTAATCTGGCCAAAAACGAGCCGACAGCTAGCTAATCACTCGAAC&#39;, &#39;CATTCTGCCAGATAGCTTCCCGCTGTTGCCCCCCACCTGTACTCTCCTTTAAAATGGCTTTGACTCTTCGGACACCTAACTAGGAGAAGGGCCGCGTCTTATTCCATGGACTTGGTACCAGCTGGCATATGCTTAAGCGCTAGAATCTCCAGTTTGCACGCCCCGGCGATACTGGAAGACGTGCTTCACGTTAGCATCTTCCGGCATTGTTGCTATTAAATAGACCAGCCGACAGTTGGATGTTTCGCCGTCGCTTCGCCCCGGCTTGAACTGAACGATTTGCCCTCGCCGCAGTTATCCGTACCTTACGACGAACTCCCTTAAAGTCATTCACTGTGGTCTGGCAGCCAAGTGCCGAATTAAGTACGGCCAGTTGACAGCTACAAGGTAAGGGCTGATCCTCTCTTATCTGCCCGTTCAGGTCATACAACCGAAGCTTGACTCATTGTGCTACGGGACCTTCGCGTCGTGCAACGTGACGGGAGGTCCTCCGGGTTAATGAGGCCCCGATTATCCAATCCAAGGGCAATGGATTCCTAAATTCCGGGGATAGAGCAGTATATACGTCTTCCAGTTCTTTTTTATCTACACGAGCAACGGACCAACACCTACTTGGTAACGCTTGGACTAAAGGGGCTGCCCTGATGTTTCGCAGAGTTACCGGGACGGCTTCGATAACTATAACTCGGTGATTTTCCTAGGACGAGTTTTGGAGTGTTACCTTATTATACCCGATGCAGCATGTTTCCCGTACTCTGGTCGGCAACTACAATCCCTAAAGGACGCGGACTTGTGTGTTGTCTAATTCGTGAATGCGACTGATCCGC&#39;, &#39;CGGCAGGAACTACGCCACGACGCGTATATAATAAAGCTTGCACTATGCAACGGTAGAATTGTATAGGCGGCCATTCCTTTCAGACTAGCCGTGTGTGCTGAAACACCTAGCTCAAGAGTTGCAAGCAGCTGCAAGGGCTAGGGATGATATGCTTAGGAGAAGTTGCGATGAACAGACATGGCGCGCAGCAGCCTATCTCACTAGTTCTGTCCGACCACTATTGTCTACATTAGGATCCAATGCGCTCACTATGGTCACCCTGGGGGCATCACCGAGCTCGATCTGATCCATGTATTAAGATCAAGAGGAACCTGAGTGGCCGCAAGCTGCATATAATATGTATGGCTAAGAATGGTGGAACTTAACTCTCAGTCTCGGACAGCCTACTGTGTATGAGAGCGTCCACAATCGTCGGGCCGCTTAAGGGCCTTTCACCATACGGGCATATCACAAATATCGACATGGACGTTATTAATCCATATCAACGACCAGAAATGATTGGGGTAATTACGGAACTATGCTCCGGTGAATTGCAAGTTGTCCCAAAATATCCTAGTCGTGGCAAGTACTAGGCGCATTTCAAAATCGGGTGAGTGGCTTGACCCGGGCTGCTTACTCAGCACTTTGGGCACGGTAAGATCCGTCATCAGCCAGTACTACCTAAGCGACGCTTGGCCGATTGCTCAGAAACCTCATGAGTATAGCACCCGGTCGAAGTTGCCTCGCGCAGACGGTCAGTCCGGACGGTTCGTGCTTCAGCGTTCACACTAACCGAGCTCCCGGTCACGACGCCTGGGCGAGAGCTCGCATGATGAGCTAACCGGCGTCCTAGCAAAAGGCACTTGAGCTAGTTCGTGGGGAAGGCGTGCGTAGTGTTCCCTCTTCACTTGCAACTTGCTCATACGAGATTAGAGTGACCATAATA&#39;, &#39;GTAGTAAACAAAAATACCTACGCATTTATTCCGAGCACACCTCGGGGGTCGATGGCGGACCCTACACCCGCTCCGTGCGTCATAGCTCTTAAATCGTATGTAGGGTCAGTTAGTGGCGCCTACTCTCTGATATGTCTTTTGTCGGTATGCACCTTTAGCCACCTTAGGAGTACGCGTCGGTTTTATACACGTGGGGAGACTAACTGCATTTTTTGCCGCCGAGGTTAAATACATAGGGAGTTCGCCGTAAGATCCCACTAATGACAAAAGGATCTAGCGTCCACTTGGGGAACTCTAGTCGATAGGCGGTTTCCGACTGTCGATCCTCCTTGGGAGAACACTAAGGCGACTTCAGACCCATTCTGAGAGGCGGGAAATCTGGAGGAAGGACATCTAGGTGTCAATGAGTGAGCTCTCCCCCTCCCCTTCCTTCGCGACTTCTTATTTAGTGCCTATGCCCTAACATTGCACCAGTTCCGTTTCAAGTTTAGCTCAGCAAAGTGTGGCATGATGTCACACCGGACAGTGAGCCGAGAACTGGGAAGCAGTTTGTGGGCAGTCGAACACCGGGACAAGGGCTTAATGAGCCGAAAACGACATGGTTCGGTCGATCCTAACATACGTATCACGGCCCCCGGCCATTTGATCTCCGTCCTGTCGACGTAGAACTCCGCTTTGTTAAAGTGTATGTCGCGTCCCGCCGTGGGGGTAGGAGTCGACCACAGTGTGAGGTCATGTACGCGTTAGGAGGTACTAGTAGTTTTCCTGTGGATGGCCTGCCTGATCCATTAAAGGAGTCTAACCTCTCATAGCCCCGTGTGGTGCAGATCGTCGAACTGCCGTGACAATCGTACGACCTCTCTGGTTAAGCGTCCCAGAAAGGGGGTATAAGGCGCATCTAAACACT&#39;, &#39;CTTTGTAGCGGCTCGTTCATCGTATGGAGTATGTACCAGAAAACCGAAGGACCCGTGGGGTGTACACGTACGGCTAACGACCCCGCGATTACTATCATCAATTAGAAGTAAGCTTCAGTTTAACAAGCTAGGTCGCAGTCTCTGTTTGAGTCTACCCAACAACCACATTTTCGATGCTTAGTTGTTTATAGGCCAGACGGTGTGCCACGGTAGAAGCAGGTTCTGCGGTTTAGACTCCATTGCAGACTCTGCTGTAGCCGACGCAGCTGTCCTCAACCGATTCCGCCCGTCTTTCGAAGATCTAAACACAGACACGCTTATAACGGAGTTCAGGATAACTGTCAAGAGTCGACCACTTTAGTTCCTCGATCTCGACCCATGGGCCTCACCGACTAGGCGATCGCAAATTGTGAGTTGGTTGTGGACCAGGAGAGTCGCTGGTTTCATCGAGCCCTCCCGTTTTCTTTACGAGTGTCGAGTAGAAAAGTCGCTATGCATTGCGTCTAACGCCTTCTGGGTAACACGAGCAACATTCCCTGCATAATTTGAGTGCGTTTTGTAAACTGACACATAGCTTAACCAATAGAGGCGTCAAACTTTTGAACTTTTCCTCGGTGGCCGGTGAGGTTCCCCGACCACTGCCTTCCGAGGAACACGAAAGGGAACTCCCGCGACTCCACGCAGGGCCAGGCTCCTTTAAGGCGGACTTCGGCTGTACCAGAGAATGTAGAATTGGACGCAGGAATTCCGCGGCCAATGATCAGTATGTCGGATCTGCACTCCGACCCCGACAACATATCCCTCTATACAAATGGATCTGCCGCAACTATGGTCATCTACTACTGTCAGTTGTGGCCTCATTCTTAAAGGACTTTGAAATGCATGGCGAGGAGAAAACATACTAGGAACTGATCG&#39;] cg = [] for i in seq: cg.append((i.count(&#39;C&#39;) + i.count(&#39;G&#39;)) / len(i) * 100) print(cg.index(max(cg))) ## 4 print(id_seq[4]) ## &gt;Rosalind_0242 print(cg[4]) ## 51.70893054024256 6.1.6 Q6: Counting point mutations def ham_distance(string1, string2): count = 0 for c1, c2 in zip(string1, string2): if c1 != c2: count += 1 return count with open(&#39;data/rosalind_hamm.txt&#39;, &#39;r&#39;)as f: t = [] for line in f: t.append(line[:-1]) ham_distance(t[0], t[1]) ## 504 6.1.7 Q7 Mendels first law def kmn(k, m, n): total = k+m+n prob = [] #possible events for k prob.append(k/total * ((k-1)/(total-1))) prob.append(k/total * ((m)/(total-1))) prob.append(k/total * ((n)/(total-1))) #possible events for m prob.append(m/total * ((k)/(total-1))) prob.append(m/total * ((m-1)/(total-1))) prob.append(m/total * ((n)/(total-1))) #possible events for n prob.append(n/total * ((k)/(total-1))) prob.append(n/total * ((m)/(total-1))) prob.append(n/total * ((n-1)/(total-1))) #m * m prob[4] = prob[4]*(3/4) #m * n prob[5] = prob[5]*(2/4) #n * m prob[7] = prob[7]*(2/4) #n * n prob[8] = prob[8]*0 return sum(prob) kmn(28, 20, 24) ## 0.7795383411580594 6.1.8 Q8: Translating RNA into Protein def trans_protein(string): #make translate table table = {&quot;UUU&quot;:&quot;F&quot;, &quot;UUC&quot;:&quot;F&quot;, &quot;UUA&quot;:&quot;L&quot;, &quot;UUG&quot;:&quot;L&quot;, &quot;UCU&quot;:&quot;S&quot;, &quot;UCC&quot;:&quot;S&quot;, &quot;UCA&quot;:&quot;S&quot;, &quot;UCG&quot;:&quot;S&quot;, &quot;UAU&quot;:&quot;Y&quot;, &quot;UAC&quot;:&quot;Y&quot;, &quot;UGU&quot;:&quot;C&quot;, &quot;UGC&quot;:&quot;C&quot;, &quot;UGG&quot;:&quot;W&quot;, &quot;CUU&quot;:&quot;L&quot;, &quot;CUC&quot;:&quot;L&quot;, &quot;CUA&quot;:&quot;L&quot;, &quot;CUG&quot;:&quot;L&quot;, &quot;CCU&quot;:&quot;P&quot;, &quot;CCC&quot;:&quot;P&quot;, &quot;CCA&quot;:&quot;P&quot;, &quot;CCG&quot;:&quot;P&quot;, &quot;CAU&quot;:&quot;H&quot;, &quot;CAC&quot;:&quot;H&quot;, &quot;CAA&quot;:&quot;Q&quot;, &quot;CAG&quot;:&quot;Q&quot;, &quot;CGU&quot;:&quot;R&quot;, &quot;CGC&quot;:&quot;R&quot;, &quot;CGA&quot;:&quot;R&quot;, &quot;CGG&quot;:&quot;R&quot;, &quot;AUU&quot;:&quot;I&quot;, &quot;AUC&quot;:&quot;I&quot;, &quot;AUA&quot;:&quot;I&quot;, &quot;AUG&quot;:&quot;M&quot;, &quot;ACU&quot;:&quot;T&quot;, &quot;ACC&quot;:&quot;T&quot;, &quot;ACA&quot;:&quot;T&quot;, &quot;ACG&quot;:&quot;T&quot;, &quot;AAU&quot;:&quot;N&quot;, &quot;AAC&quot;:&quot;N&quot;, &quot;AAA&quot;:&quot;K&quot;, &quot;AAG&quot;:&quot;K&quot;, &quot;AGU&quot;:&quot;S&quot;, &quot;AGC&quot;:&quot;S&quot;, &quot;AGA&quot;:&quot;R&quot;, &quot;AGG&quot;:&quot;R&quot;, &quot;GUU&quot;:&quot;V&quot;, &quot;GUC&quot;:&quot;V&quot;, &quot;GUA&quot;:&quot;V&quot;, &quot;GUG&quot;:&quot;V&quot;, &quot;GCU&quot;:&quot;A&quot;, &quot;GCC&quot;:&quot;A&quot;, &quot;GCA&quot;:&quot;A&quot;, &quot;GCG&quot;:&quot;A&quot;, &quot;GAU&quot;:&quot;D&quot;, &quot;GAC&quot;:&quot;D&quot;, &quot;GAA&quot;:&quot;E&quot;, &quot;GAG&quot;:&quot;E&quot;, &quot;GGU&quot;:&quot;G&quot;, &quot;GGC&quot;:&quot;G&quot;, &quot;GGA&quot;:&quot;G&quot;, &quot;GGG&quot;:&quot;G&quot;,} # Find first instance of AUG (start codon) start = string.find(&#39;AUG&#39;) #split string into a list string_lst = [string[i:i+3] for i in range(0, len(string), 3)] #find the first instance of stop codon end = [] for i in [&#39;UAG&#39;, &#39;UAA&#39;, &#39;UGA&#39;]: if i in string_lst: end.append(string_lst.index(i)) else: end.append(0) #translate strings to aminoacids string_lst = string_lst[0:(max(end))] amino_seq = &#39;&#39; for i in string_lst: amino_seq = amino_seq + table[i] return amino_seq with open(&#39;data/rosalind_prot.txt&#39;, &#39;r&#39;)as file: line = file.readline() print(trans_protein(line)) ## MAEFLSWMSSQLVTVCTRGGHRYWWGHGENPGRSRLAAHLYTLHSSYPFDRIARKQLAVLNSRAAQPRLLYRFAPINVRHLNLLNRPTLYGVDSWMRSAFSGSACSFMWTSTGGIVTIASSATACMPQANKRLQWFITYAASALIGIVSCTAPTARELFSTSLVPHRTISEYWRSRTGFLLFVVYMPLRGSTEGCGTSTTCSWKYKNLWMRDDDQQEVGCFSNPFSDTNFHVRRCAQHFLDGDPFLNYSTLPGSLLLKDKTLTVQHRRRLRLDIRGNRQQVGRLACSSKLRPLSPPPPILANCRMVWNVNIHPCNRKLDPRVMTSVNNLDPWPDFSDLRFSCPPSRSDEGENLQHKGGIYASLNQAVVGNTLDPGPNAIRIWSTACGIIRLKPYLLGQLLGTRRKTQSRGNQGRRCAQYQQAISVLIPIGTDPVGRAPSAMIRDRCKNRRPAEVVQVNRANHDRCDYLNIYILAHRLLLTARAEVMADCCTELRATFPSGAHCQPGNLPCRVKVVRSILSSKLSVLVPRNSATRELTEKRCIGEFHGRHRYPKPSRGYHHLGHSLRFLCGLAGIAQRFLYRARKFEYAYVRLRGLLTGIYRETLNDLLATICYVSRAYILCPTTCVERLCQRTGVTHQYSSGNSQVRHPSKRFPQNERLSNCFESSNYPFEQYGPLRPIATISGRTLVKENMRCPQVVGAVCFTEMIMLEHKCEGLSQPRLPLLIRGLLPTVIHLTQQDSRAGMVWVEIRWRGWRLNGRWVREPRVYRRAYDARAPVSRTVVSAWSTHSIAPRSALDARIGANCFGHLDGFHNQCILSSSKFLAQITRQRKASRGSWHPSTTAPIAYTSKPVIIRPCGRCSRQWGAVLAFAQVEAGLIPISCLFIRVTLTSAGRSRLKLCRSANRSPRERTSAESKARTYHTPRILFRPRSARRAYNCKFYVVHSELMCGHRGIIPSNSLGYKWRTYQTGIEYDHGLVRTSALDFYPCFLNDCTPLTGVGEDMKASGTDQRGIHRSVFAEFNRSAYYGYHDPGVCPFATIVPTGLVDTMANYTRCAQETGDGYLGTSANSNGKYYYQACFPFDFTTRREGRASSLAVSDNFGRLCSPIPTPIVILFPASRLGDCEACILKRLVHERCCNPDVSSLFQHNLCSTHRQVAREATAYGGASRTTTEEAWACRRRGDIFQWLLRDSFLRLHCSYLLPIKDMLYKEYRLTLLGISFFNYSPGLINHHSGSRTCGRHRDTHSSYRAEYVRLNYRPTFLTNRELRAWFTFAVCRITASAGIHHIGEAQSAGGYHRANSKQCYIALGRVEGRFCDPGECPIPDRGRTIKGLSTCVILSLLVLRRAGHVMRHPRPIGVEPYSADLAPIRADITYSLHFMTVISNIDQCPDLCFMRIRSLRKGPRPQLKTGYLYESLNQLSVHCSSSATDVPVTFTITRETVACTLLRPTAETTRCGASGPLRRESRRSSGSWKQAYQTNILPEASLTLIPQADPISNKRRSVRLFCLVRKPQQEYLSVLLFISHLTLSKLRRMKWELLSEKYMSSKGPFIVLDSGCMRPPSVIQPSNSLSIVILQATTSCWRSNSEGRGLGLIEYDTSCDLHTLTARSQGRLPPLRRAPCYNIVIGELNEIWLALQFTCILSAIVPDQGYSKKSYRLVRCYDYLEHDVRRCHMTCEHSSLSERRIPRNLQGSPDHYMPGRNLSNCRSHILLLNIGDRVTGLQLTHYMVTNQFTRSETYFSGASDRPCNVAISGLFSHVPSSCLTWPLPLPAVDRRLLRSTFGMLLVLLGLWVYPSVVVVRYFPGRLCSAWYSRRAVSNGIWGKFLEGHRTGVITGSLKLLFQQQPVYARGFFTSTYSSPMVAPAYYAIVGCASMSEPLRIALHAISMNGLQRWGHEAELGTAHFCSTRSPPVCLPRTAVGGEENVRHSMDQFWLHLRVGPGPGMGWCALDRLHLFHKSTTGRLITRAWEYLSVSVLRFRCHRPGPTGPPPQEQSAVLITSRKRTMTAETLISINHASAVEARLLTCLATNRQYVLLNFGNVVIMDVKNWSLMATHIEHLRYVSEDRATLPRDRQIPGRFTLPWKHSHPGKAVIVLDLYLRMHLSAVIEQFAIQLCGSGPTCKMDSQVAALYAQTTFTRPWNTSADGTSYVLCRLNQQLRVRNSIKLGCSAMVGSTPLTLRGVNFVLTIPFAKGSTRPRAHHRSPHQVGSGSPMRCRPSEGRVRVALPEAFAFVYSMLVYQRATELSDSSQDNEVSQMRYAKVARYKLRRALHLRSVFIRETRLIPIAKYDDSPLAEAGDCPYGDNLHFAIGTLGPAPRRVDGNGGIDIQVRLRNAKSVDLPGLSVVLKACQGGLAISDGYFFLSITGTPTPDDWARRVAARYTTAPEAAFCKNRVLFDLQSTNGDRVDSSRYTFHISEAERKSQDTISFSVSPLEASGRNQRASFLEWPDSFPHLVEPTNVYLGDSSATLLHIPRRRSRLSPRHYIGAKILTRQVVPEMSLVGSRRELPVTQNLGNRSDNETLSGVHALHDPAFALRLGQLVSTQQRRGRLFTECSACICADRWGLTQVGPITTLDYTEWRGRPYIRKITNSSGGMVLDDGRGRRGVSRAEHETRILRDFLDVSRINTMWNYKVMLLNSSYQQGSKGDCCHAPAVWLVLVPLPSKSVNHYKAQSTLQAARTPLAFPGSGPRLDTHVVIILTNHLPAFSRLNSRSQCAGSKDHVLFTPTLFRQLRPNISVLSLGQQSMQGYGLNSVWQFDPNRSLSMHLYELPGCHVTESENGGQDRVPTHSALCQVQRIANVLTIMTRTALMVIDFSRVECQPQGRRNQVNFSKGRLMHRNTCSIAFTLLLTFRDGEDQRARVGTKNRSRVTTMSDWIKNRSHSDYLV 6.1.9 Q9: Finding a Motif in DNA with open(&#39;data/rosalind_subs.txt&#39;, &#39;r&#39;) as dna: string = dna.readline().rstrip() motif = dna.readline().rstrip() index_start = [] y = 0 x = 0 while x != -1: x = string.find(motif, y) if x != -1: index_start.append(string.find(motif, y)) y = string.find(motif, y) + 1 #adding 1 to all list elements for i in range(len(index_start)): index_start[i] = index_start[i] + 1 print(*index_start, sep = &#39; &#39;) ## 15 84 93 100 107 114 122 146 193 218 234 267 343 411 433 477 507 583 609 619 653 715 722 763 771 833 866 6.1.10 Q10: Consensus and Profile def consensus(seq): &quot;&quot;&quot;Creates a consensus sequence out of a list of sequences. input = List of strings output = consensus sequence and nucleotide count of input sequences example: ------------------ ATGCAACT A: 5 1 0 0 5 5 0 0 C: 0 0 1 4 2 0 6 1 G: 1 1 6 3 0 1 0 0 T: 1 5 0 0 0 1 1 6&quot;&quot;&quot; #make a nested list of seq seq_list = seq # print(seq_list) array = [] for i in range(len(seq_list)): array.append(list(seq_list[i])) # print(array) profile = &#39;&#39; for i in range(0 , len(array[1])): for a in array: #collect all nucleotides of the diffrent sequences, store in string profile = profile + a[i] profile_lst = [profile[i:i+len(array)] for i in range(0, len(profile), len(array))] # print(profile) # print(profile_lst) #count A T C G, store counts in lists A = [] C = [] G = [] T = [] for i in profile_lst: A.append(i.count(&#39;A&#39;)) C.append(i.count(&#39;C&#39;)) G.append(i.count(&#39;G&#39;)) T.append(i.count(&#39;T&#39;)) #create consessus profile = [A, C, G , T] chunk = [] consens = &#39;&#39; for i in range(len(A)): for n in range(4): chunk.append(profile[n][i]) consens = consens + str(chunk.index(max(chunk))) chunk = [] #translate interger into dna string: table = {ord(&#39;0&#39;):&#39;A&#39;, ord(&#39;1&#39;):&#39;C&#39;, ord(&#39;2&#39;):&#39;G&#39;, ord(&#39;3&#39;):&#39;T&#39;} consens = consens.translate(table) #format output print(consens) print(&#39;A:&#39;, &#39; &#39;.join(str(i) for i in A), &#39;\\n&#39; + &#39;C:&#39;, &#39; &#39;.join(str(i) for i in C), &#39;\\n&#39; + &#39;G:&#39;, &#39; &#39;.join(str(i) for i in G), &#39;\\n&#39; + &#39;T:&#39;, &#39; &#39;.join(str(i) for i in T)) A test with an example dataset: consensus([&#39;ATCCAGCT&#39;, &#39;GGGCAACT&#39;, &#39;ATGGATCT&#39;, &#39;AAGCAACC&#39;, &#39;TTGGAACT&#39;, &#39;ATGCCATT&#39;, &#39;ATGGCACT&#39;]) ## ATGCAACT ## A: 5 1 0 0 5 5 0 0 ## C: 0 0 1 4 2 0 6 1 ## G: 1 1 6 3 0 1 0 0 ## T: 1 5 0 0 0 1 1 6 def to_sigle_fastq(in_file, out_file, header): &quot;&quot;&quot;&quot;&quot;&quot; with open(in_file, &#39;r&#39;)as fastq_multi, open(out_file, &#39;w&#39;) as fastq_single: block = [] for line in fastq_multi: if line.startswith(header): #if block contains anything, write block to file #this only writes the seq after a seq has been collected in block if block: fastq_single.write(&#39;&#39;.join(block) + &#39;\\n&#39;) block = [] #always write header line to file fastq_single.write(line) else: #add seq on other line to block block.append(line[:-1]) fastq_single.write(&#39;&#39;.join(block)+ &#39;\\n&#39;) to_sigle_fastq(&#39;data/rosalind_cons.txt&#39;, &#39;data/rosalind_cons_single.txt&#39;, &#39;&gt;Rosalind&#39;) with open(&#39;data/rosalind_cons_single.txt&#39;, &#39;r&#39;)as file: string = [] for line in file: if line[0].startswith(&#39;&gt;&#39;): pass else: string.append(line[:-1]) consensus(string) ## ACACACAGGCATATTTGACGAGCACGCGCGCACATGAAGAAGTACTGGGCTATCCGGAACCAACCACACTGCACACCGTAGACGTTAAAGTGAGCTTCAGACAACTCGCTCCACAGTAGTTAGGCAGCACATCAGCAACAGCAAACAACCAGCACATCGCTTTACACAGCCCATGCGCAACCGCCTTAAACAAACAGACGCCACCCAAAGAGCCACTGAAAAACGTCACTCGCAGGAAATAAACTTTGCGCGGAACCAGGATAATCACTAAATGAGCTCGATGACCAAATCACCCCACCACATAGAGCCCGCACGACAAAGATCAGGCACCTAAGCGGAGATTTCAACAGAGCCCCATGAACAGTCCGCACTCTGCGATACCGATCTACGAGACCGCCCAAAAAACAATACCTTCAACGCATTATGAATGGACCGCAGAATGAATCTGCAACCCAGATGCGATCACGCATAAGCCACGACAGTGGAACAAGCGGTTGGCTCAAGATCGAAACTCGTAACAGGTACGTTGTTCGAACTAAATTTAAGATCACAGAGTTCGTGACCGACTATCTAAAAAAATCTGGTAGGGACCCCGAAGAATTAATAACAACAATAAATAACCAATCTGGAATCCGCCAGGCCCATGCGATTACTCGACAATCGCCACTTTAAAATTAAGGCGTCCAACCAGGATGGGTCTTGCCGGATGTTCGACAGACGTGAACACCGTTCTACGGCTCACACTCGTGGTCGATTAAAGACGGTCGGCGCTGTTAACGACACCTGTGCCTACGTGAAGTAGTGTCTTGGTCCGCAATTACTTTTAGACTCCATATTCTCCATTGCGCGAACCGACCTGAAGCCCCACTGCAACTACAGGATTGAACGGTCGCCCAATTAAATTTCTAAAAACA ## A: 4 3 3 3 4 2 6 2 3 1 3 3 3 3 2 3 1 3 1 2 4 2 2 4 3 2 1 2 2 2 2 3 2 4 3 1 5 5 2 3 3 2 2 4 3 2 2 2 2 2 2 5 1 2 1 1 2 4 4 2 1 3 3 2 2 4 1 5 3 0 1 2 5 2 3 3 3 3 1 3 3 3 1 3 3 0 4 3 5 1 1 4 4 2 2 1 2 1 6 1 4 3 3 5 2 3 0 2 2 2 1 3 4 2 3 1 1 4 1 2 3 6 1 2 3 5 2 0 4 3 6 0 2 3 3 1 5 3 2 4 2 2 4 4 3 1 5 3 2 3 4 3 3 5 0 3 3 1 2 1 3 2 3 3 2 4 1 3 2 0 2 2 4 0 2 0 1 1 3 5 3 2 1 2 1 2 2 3 4 4 0 4 3 4 2 5 2 3 1 1 2 1 3 0 2 1 3 4 4 3 4 1 2 3 4 1 0 3 3 4 3 4 3 2 2 3 1 4 2 1 2 2 0 5 3 1 5 3 4 1 4 3 4 3 2 3 1 3 1 1 1 2 2 6 3 2 1 3 2 1 3 3 5 4 2 1 5 3 2 3 4 6 2 2 4 1 1 1 3 3 6 1 1 3 3 2 4 3 3 2 1 4 3 2 2 2 3 2 3 4 3 3 1 3 1 5 2 1 2 2 2 2 3 1 0 3 2 5 5 4 1 5 0 3 3 3 2 2 3 2 0 3 5 3 2 2 0 2 3 1 4 2 2 3 2 4 4 3 3 2 5 2 2 3 0 0 3 2 2 5 5 3 3 2 1 0 3 2 2 3 2 3 1 2 2 0 2 4 1 4 1 2 1 4 3 2 2 4 0 1 4 1 5 2 3 1 2 2 3 5 4 3 3 5 3 1 3 4 3 3 3 2 3 1 3 3 5 2 2 0 3 2 3 3 2 2 4 4 1 3 2 4 3 1 2 2 4 1 4 4 2 3 3 5 2 1 3 2 2 3 3 3 1 1 5 2 4 0 1 3 2 4 2 0 4 2 3 2 3 2 5 4 1 1 3 3 1 0 5 3 3 1 2 3 2 5 4 1 5 4 2 3 1 0 2 3 1 2 3 1 1 4 4 1 4 2 3 3 3 4 4 1 3 2 1 1 4 3 1 4 3 0 2 3 2 2 3 3 0 2 2 1 3 4 4 1 3 3 5 3 2 0 3 3 5 1 3 3 2 3 2 6 3 3 2 0 0 2 1 2 1 3 2 2 2 4 2 3 3 3 2 1 3 4 3 3 3 3 4 3 1 3 2 2 1 3 3 2 1 4 2 2 2 1 3 4 4 1 5 5 3 2 3 5 0 4 3 3 3 3 1 5 5 3 5 3 3 1 4 3 2 0 4 5 2 3 3 1 0 4 4 3 2 1 3 2 1 3 0 2 2 2 2 5 2 2 3 3 4 2 1 3 1 0 2 0 6 0 3 4 1 1 2 2 1 5 3 3 3 1 3 6 3 5 3 3 3 4 4 1 2 3 3 0 1 4 3 0 1 4 3 2 3 2 2 2 2 2 2 2 1 2 1 2 2 4 3 2 1 3 2 2 1 4 1 4 1 4 2 0 2 1 4 4 1 4 2 2 1 2 0 1 1 4 1 3 1 2 1 2 4 3 5 1 2 2 2 1 0 2 1 2 4 3 1 2 4 4 3 2 4 2 3 2 2 2 1 1 2 2 0 2 2 1 1 4 4 0 0 3 0 5 1 2 1 1 3 2 3 2 1 4 3 2 3 1 3 5 3 1 3 2 2 2 1 1 2 0 2 2 2 2 2 1 2 4 5 2 1 4 2 2 1 1 2 4 2 5 1 2 2 2 4 1 4 2 3 1 3 1 2 3 1 2 2 2 3 1 3 3 4 3 1 2 3 1 1 3 2 5 4 3 2 2 0 1 3 1 4 1 3 3 4 3 0 5 2 4 3 2 5 2 3 1 4 5 0 3 3 1 2 1 1 3 2 4 3 0 1 4 3 4 0 1 0 1 1 4 6 4 3 3 2 4 ## C: 4 5 3 5 3 3 0 1 2 4 3 2 2 2 1 1 4 2 5 2 1 0 3 3 4 1 4 1 3 3 4 3 4 2 1 2 3 1 2 3 2 3 0 2 4 2 2 2 2 5 3 2 2 4 4 2 2 1 1 6 3 3 3 5 3 3 5 0 5 3 1 4 1 4 2 4 5 2 3 2 2 2 5 3 0 2 0 3 3 3 2 0 2 2 4 2 2 4 0 1 1 4 2 0 5 2 4 2 3 0 3 4 1 4 2 3 3 4 1 3 1 2 3 3 5 4 0 5 1 6 1 2 4 1 0 4 1 2 4 3 0 4 1 3 3 4 0 3 4 4 3 2 4 2 4 1 2 4 3 4 2 3 0 1 3 1 5 3 1 5 5 5 0 3 2 4 3 5 3 4 4 5 2 4 6 1 2 1 3 3 4 2 2 2 4 2 1 2 4 2 6 5 2 7 4 3 1 4 3 2 2 1 4 4 2 5 3 2 2 2 1 2 1 3 2 2 4 3 5 3 3 2 4 3 1 2 0 2 3 2 2 3 1 4 0 0 2 1 4 2 3 2 2 0 2 3 4 2 3 3 3 1 3 2 2 5 2 4 1 3 3 3 1 2 2 3 4 2 4 2 2 3 2 1 4 3 3 1 3 1 3 1 4 4 3 4 2 4 4 2 4 3 2 2 1 3 3 3 4 4 1 4 3 4 3 2 4 4 2 1 1 2 0 5 3 1 2 5 2 5 4 0 1 3 2 5 2 2 3 2 2 1 2 1 3 4 1 5 3 2 3 2 5 6 5 5 3 3 2 3 0 4 2 2 4 4 5 0 5 1 3 2 3 1 2 5 2 1 3 3 4 5 2 2 2 5 0 3 5 2 1 3 1 3 5 2 4 4 5 1 4 3 3 2 3 4 2 2 2 1 4 5 2 2 5 2 3 3 2 6 3 1 1 1 1 2 4 2 2 1 2 1 4 5 1 4 3 2 1 2 1 2 2 1 1 3 3 2 5 1 2 4 5 3 1 2 1 3 3 4 1 2 3 4 3 5 2 4 1 2 1 4 2 4 6 3 4 3 2 5 3 2 0 1 2 0 2 4 3 4 2 4 1 1 3 0 1 3 4 1 4 2 3 1 1 2 5 2 2 2 1 4 3 4 2 1 3 2 3 4 2 2 3 1 3 2 2 3 3 1 3 4 0 3 1 4 2 2 2 2 2 4 1 3 2 3 2 2 3 2 6 1 2 2 3 3 3 5 2 3 1 2 5 3 2 2 4 1 2 1 4 2 1 0 2 3 1 3 1 1 4 2 1 3 2 2 1 2 2 2 3 4 5 5 1 3 3 2 0 3 1 1 2 3 3 3 3 5 2 3 3 2 1 1 2 2 2 2 1 3 4 4 2 1 1 5 2 1 3 2 1 1 3 6 0 3 6 3 3 1 5 3 5 2 1 2 4 3 2 2 3 3 4 3 4 2 1 5 2 0 2 3 2 5 6 2 4 2 2 2 3 0 2 0 1 0 3 2 0 3 5 2 1 6 4 1 1 5 4 1 1 2 1 3 1 2 1 1 3 3 2 2 4 3 2 1 2 2 2 0 1 3 1 2 6 4 2 3 4 2 1 2 4 4 5 1 4 3 2 2 3 3 3 2 4 0 3 3 3 5 2 4 1 5 1 3 2 2 2 2 1 5 1 3 2 2 2 1 2 2 2 4 1 2 2 3 2 1 3 1 5 2 2 3 1 2 2 5 1 1 4 3 4 3 3 3 2 3 4 4 3 2 4 2 2 2 3 1 0 2 3 0 0 2 3 6 1 1 0 1 2 5 4 2 4 3 2 1 3 3 4 1 0 2 2 2 2 1 3 2 5 4 2 2 3 2 1 3 1 4 5 3 1 2 1 6 1 4 1 3 2 4 6 2 2 3 4 0 2 2 3 2 3 4 4 4 2 3 0 3 4 3 3 4 2 0 3 2 1 3 0 1 0 2 2 3 6 0 1 3 3 3 4 5 4 4 3 3 3 1 3 2 3 3 1 4 1 1 2 1 1 3 3 1 ## G: 2 1 2 0 2 3 2 4 4 2 2 1 3 0 3 2 5 3 3 3 2 4 2 2 2 4 4 4 3 5 2 2 3 4 2 6 1 3 5 1 3 4 2 3 1 2 4 3 4 1 1 1 0 2 4 5 4 1 2 2 3 1 1 2 2 2 2 3 2 3 6 1 2 2 2 3 0 4 2 2 4 3 2 4 3 3 4 2 0 5 3 5 0 3 0 3 1 2 3 5 1 1 3 2 3 0 3 4 2 3 3 2 2 3 2 4 1 1 4 1 2 1 4 5 1 0 4 3 2 0 1 2 1 3 4 3 3 3 1 3 4 4 3 1 3 3 4 3 2 0 2 4 2 2 3 3 1 2 4 3 1 1 3 3 2 3 4 2 4 4 2 1 3 3 5 3 5 2 1 1 2 1 4 3 3 3 2 3 0 1 3 3 2 3 4 1 5 3 4 6 1 2 2 1 1 3 3 1 3 4 1 5 1 3 2 2 3 4 2 2 3 4 3 3 4 1 2 3 1 1 3 3 3 0 4 5 0 3 2 3 4 1 4 2 3 1 1 4 2 6 3 4 4 2 3 3 3 3 4 4 2 1 0 0 2 1 2 1 3 2 2 1 3 4 2 4 1 3 1 4 1 1 4 3 1 2 0 3 2 1 3 4 0 1 3 0 3 2 0 2 2 2 2 3 4 1 4 3 2 3 4 2 1 3 4 2 1 1 1 3 4 3 4 2 1 4 3 1 3 2 2 3 2 3 5 1 5 3 1 5 2 2 0 2 2 2 3 1 3 4 2 3 2 0 1 3 1 1 5 2 4 2 3 5 0 2 2 7 3 3 2 0 3 3 4 3 3 2 2 1 3 1 4 1 1 0 3 1 3 5 3 4 3 3 0 5 3 3 1 2 1 3 1 0 3 3 3 3 1 3 1 2 1 3 1 3 2 2 3 2 1 2 2 3 2 3 1 4 3 4 3 2 0 1 5 1 1 5 1 3 3 4 2 3 2 3 0 4 2 3 3 1 3 3 4 4 3 2 5 2 4 2 1 3 1 2 5 2 3 2 2 0 4 3 1 3 2 4 2 1 2 4 2 4 3 5 3 3 1 2 3 0 4 5 1 3 6 4 1 3 3 2 3 6 3 2 1 4 2 2 3 3 0 1 4 3 3 2 3 0 4 7 0 3 3 4 0 0 4 3 1 2 4 3 3 3 1 2 2 2 1 1 2 3 1 4 2 1 3 3 2 0 5 3 4 2 3 0 5 1 6 2 1 3 5 1 1 2 2 1 2 3 3 2 3 1 3 2 2 2 3 0 4 4 3 3 4 4 6 2 2 2 1 4 6 2 2 4 2 2 1 2 2 2 3 3 2 1 3 3 3 1 4 2 1 2 2 3 3 3 1 2 3 2 2 0 1 4 4 2 2 1 3 1 4 3 1 1 5 5 2 3 1 2 3 4 2 4 1 1 1 1 3 3 1 4 2 2 2 2 3 3 4 1 2 1 2 1 1 3 3 4 3 3 2 2 2 2 5 4 0 4 1 3 4 4 3 3 2 4 4 3 3 1 4 3 4 2 2 1 3 3 2 3 3 5 2 2 5 3 3 3 8 3 2 1 6 1 4 5 2 6 1 1 1 2 3 2 4 2 1 3 2 1 1 6 6 2 2 2 2 2 1 4 3 2 3 2 5 3 3 0 5 3 2 2 2 2 3 3 1 3 5 3 2 2 5 4 3 5 5 2 5 2 2 2 2 3 5 3 2 0 2 2 1 4 1 4 2 1 2 2 1 4 1 5 2 3 5 3 2 5 2 4 2 2 3 3 5 4 2 1 1 5 1 2 1 2 1 1 1 2 4 3 2 2 4 1 3 2 2 3 4 2 1 2 2 3 1 2 1 1 2 2 4 2 4 2 4 2 2 2 0 3 3 3 3 3 4 2 2 4 3 2 4 2 3 3 0 4 2 3 3 0 3 4 2 2 4 4 4 0 1 4 3 0 3 6 5 2 3 5 1 2 3 2 1 3 1 3 2 2 3 0 3 2 2 3 0 2 3 1 3 2 ## T: 0 1 2 2 1 2 2 3 1 3 2 4 2 5 4 4 0 2 1 3 3 4 3 1 1 3 1 3 2 0 2 2 1 0 4 1 1 1 1 3 2 1 6 1 2 4 2 3 2 2 4 2 7 2 1 2 2 4 3 0 3 3 3 1 3 1 2 2 0 4 2 3 2 2 3 0 2 1 4 3 1 2 2 0 4 5 2 2 2 1 4 1 4 3 4 4 5 3 1 3 4 2 2 3 0 5 3 2 3 5 3 1 3 1 3 2 5 1 4 4 4 1 2 0 1 1 4 2 3 1 2 6 3 3 3 2 1 2 3 0 4 0 2 2 1 2 1 1 2 3 1 1 1 1 3 3 4 3 1 2 4 4 4 3 3 2 0 2 3 1 1 2 3 4 1 3 1 2 3 0 1 2 3 1 0 4 4 3 3 2 3 1 3 1 0 2 2 2 1 1 1 2 3 2 3 3 3 1 0 1 3 3 3 0 2 2 4 1 3 2 3 0 3 2 2 4 3 0 2 5 2 3 3 2 2 2 5 2 1 4 0 3 1 1 5 6 6 2 3 1 3 2 2 2 2 2 2 2 1 2 2 5 2 4 4 3 1 2 4 2 1 0 4 2 2 2 4 4 2 1 1 5 3 3 2 3 3 3 2 6 3 1 3 3 2 4 2 2 3 2 1 2 5 2 4 1 1 3 2 1 3 2 3 2 3 3 3 0 2 2 4 0 6 0 3 2 3 2 2 1 4 4 2 1 1 2 3 3 3 2 2 5 6 4 3 0 2 1 1 2 0 3 1 1 4 2 3 4 1 0 1 1 2 1 5 4 0 1 0 3 3 5 3 4 2 2 3 3 4 2 2 2 3 3 4 3 5 2 2 2 2 2 1 2 2 2 1 1 1 2 1 1 3 3 1 2 2 1 4 3 2 1 4 4 1 2 0 3 3 2 3 5 4 3 5 3 1 0 4 2 3 3 3 3 2 3 2 2 4 1 4 1 3 1 5 3 4 2 1 3 2 2 1 3 0 2 2 5 1 1 3 2 4 3 2 1 0 2 3 4 2 2 3 2 0 1 3 3 1 1 2 3 6 2 3 0 1 2 1 0 3 3 4 4 4 4 2 1 2 5 2 2 0 2 2 4 1 1 3 2 2 2 4 3 3 5 0 3 3 2 1 1 5 3 2 2 5 4 3 4 4 3 3 0 2 2 4 3 1 3 5 5 4 1 2 2 3 4 2 2 0 3 0 2 1 5 4 3 2 4 2 3 2 2 1 3 3 4 3 5 2 4 3 4 2 3 3 2 3 4 2 5 3 1 4 2 2 2 1 2 3 2 2 0 0 1 1 3 3 0 5 5 3 0 4 0 2 1 2 1 3 2 0 4 2 3 3 4 2 1 3 4 1 2 5 2 4 4 3 2 3 5 2 2 3 2 2 3 2 2 1 2 2 1 4 2 1 0 3 5 5 3 2 4 3 4 1 3 3 4 4 3 2 2 1 2 1 4 4 4 1 0 2 2 4 5 2 2 1 2 3 1 5 1 1 1 3 2 3 1 2 3 3 4 3 3 3 5 3 4 4 3 3 2 3 0 3 4 2 4 4 2 0 1 1 1 1 2 0 3 5 1 1 1 3 3 1 3 3 4 6 3 4 3 4 1 0 3 4 1 2 1 3 0 4 3 3 5 3 3 5 3 0 1 5 4 2 3 2 3 3 1 1 3 4 3 2 4 2 2 0 4 1 4 6 2 2 2 4 3 4 2 3 3 5 2 4 1 1 3 4 2 2 2 4 2 2 1 2 4 2 3 6 2 4 1 4 6 3 3 4 2 3 2 3 1 2 5 5 2 3 5 5 4 4 2 2 3 3 4 1 1 0 5 2 4 4 3 5 3 2 3 6 4 3 0 2 3 2 2 2 1 3 3 2 3 2 4 2 1 1 1 2 2 2 3 2 3 6 2 1 1 0 3 5 1 3 2 2 1 1 7 6 3 1 2 1 1 1 4 2 1 4 0 1 0 3 4 5 2 2 2 4 6 6 3 6 2 2 3 3 3 2 3 "],["influenza-activity-across-europe.html", "7 Influenza activity across europe 7.1 Loading and colapsing data 7.2 Storing new dataframes 7.3 Joining dataset 7.4 Visualizing data", " 7 Influenza activity across europe Influenza, commonly called the flu, is an infectious disease caused by The influenza virus. Symptoms can be quite mild; fever, runny nose, sore throat, cough, headache, muscle pain and fatigue. Or severe pneumonia, which can sometimes be attributed to a second respiratory infection by a bacteria. Influenza has a history of epidemic/pandemic outbreaks. There are seasonal epidemic outbreaks of influenza during the winter months when the humidity and temperatures are low, which favors transmission. There have also been four pandemics outbreaks of influenza since 1918. These pandemics happen when a Influenza strain from either an avian or swine population spreads to humans. The resulting strain is antigenically different from existing from previous circulating strains. Thus resulting in a fast transmission through a population which has yet to build up immunity. (Krammer et al. 2018) Dengue fever, commonly called break bone fever is a disease caused by the dengue virus. It is transmitted by mosquitoes in tropical environments around the equator. Symptoms can be fever, headache and vomiting or in sever cases dengue hemorrhagic fever can occur, which is when the vascular permeability is increased causing a decrease in blood pressure. (Fever, n.d.) library(tidyverse) #1.3.1 library(readxl) #1.3.1 library(here) #1.0.1 library(knitr) #1.33 library(captioner) #2.2.3.900 library(dslabs) #0.7.4 library(DBI) #1.1.1 library(RPostgreSQL) #0.6-2 library(remotes) #2.3.0 library(RPostgres) #1.3.2 library(hrbrthemes) #0.8.0 library(viridis) #0.6.1 library(kableExtra) #1.3.4 source(here(&quot;src/port_src7.R&quot;)) 7.1 Loading and colapsing data The files dengue_data.txt and flu_data.txt are orginate from goolgle. Data Source: Google Flu Trends. the gapminder data comes from the package {dslabs} dengue &lt;- read.csv(here::here(&quot;data/dengue_data.txt&quot;), skip = 11) flu &lt;- read.csv(here::here(&quot;data/flu_data.txt&quot;), skip = 11) data(&quot;gapminder&quot;) The data for influenza and dengue activity is not in a tidy format. Also the data in the gapminder dataset is per year, while the data for influenza and dengue activity is per week. By collapsing the data for flu and dengue activity the joining of these three datasets will be possible. dengue &lt;- dengue %&gt;% pivot_longer(Argentina:Venezuela, names_to = &quot;country&quot;, values_to = &quot;value&quot;) flu &lt;- flu %&gt;% pivot_longer(Argentina:Uruguay, names_to = &quot;country&quot;, values_to = &quot;value&quot;) dengue &lt;- dengue %&gt;% separate(&quot;Date&quot;, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) dengue &lt;- dengue[-c(2,3)] #removing month and day dengue$year &lt;- as.integer(dengue$year) dengue$country &lt;- as.factor(dengue$country) dengue &lt;- dengue %&gt;% group_by(year, country) %&gt;% summarise(&quot;country&quot; = country, &quot;dengue_activity&quot; = sum(value, na.rm = T)) %&gt;% unique() flu &lt;- flu %&gt;% separate(&quot;Date&quot;, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) flu &lt;- flu[-c(2,3)] #removing month and day flu$year &lt;- as.integer(flu$year) flu$country &lt;- as.factor(flu$country) flu &lt;- flu %&gt;% group_by(year, country) %&gt;% summarise(&quot;country&quot; = country, &quot;influenza_activity&quot; = sum(value, na.rm = T)) %&gt;% unique() Table 2: The first ten rows of the resulting dataframe after collapsing all datapoint of each week into a total per year, and making the data into a tidy format. year country influenza_activity 2002 Argentina 0 2002 Australia 0 2002 Austria 0 2002 Belgium 0 2002 Bolivia 0 2002 Brazil 174 2002 Bulgaria 0 2002 Canada 0 2002 Chile 0 2002 France 0 7.2 Storing new dataframes #storing dataset locally for (x in c(&quot;flu&quot;, &quot;dengue&quot;, &quot;gapminder&quot;)) { export(get(x), path = paste0(here(&quot;output//&quot;), x)) } #storing datasets on Dbeaver dbWriteTable(con, &quot;dengueDB&quot;, dengue, overwrite = T) dbWriteTable(con, &quot;fluDB&quot;, flu, overwrite = T) dbWriteTable(con, &quot;gapminderDB&quot;, gapminder, overwrite = T) Figure 1: The three dataset stored in a dbeaver database Checking if the datasets are accessible: SELECT * FROM &quot;dengueDB&quot; WHERE country = &#39;Argentina&#39;; Table 7.1: Displaying records 1 - 10 year country dengue_activity 2002 Argentina 0.000 2003 Argentina 1.397 2004 Argentina 1.533 2005 Argentina 1.035 2006 Argentina 1.007 2007 Argentina 5.569 2008 Argentina 1.462 2009 Argentina 16.739 2010 Argentina 6.621 2011 Argentina 3.186 SELECT * FROM &quot;fluDB&quot; WHERE country = &#39;Netherlands&#39;; Table 7.2: Displaying records 1 - 10 year country influenza_activity 2002 Netherlands 0 2003 Netherlands 786 2004 Netherlands 1014 2005 Netherlands 1779 2006 Netherlands 1497 2007 Netherlands 1607 2008 Netherlands 1801 2009 Netherlands 3062 2010 Netherlands 1145 2011 Netherlands 1627 SELECT year, population, life_expectancy FROM &quot;gapminderDB&quot; WHERE country = &#39;Netherlands&#39; AND year &gt;= 2002; Table 7.3: Displaying records 1 - 10 year population life_expectancy 2002 16076427 78.5 2003 16167421 78.7 2004 16253397 79.1 2005 16331646 79.6 2006 16401105 79.9 2007 16463031 80.2 2008 16519862 80.3 2009 16575173 80.6 2010 16631571 80.8 2011 16689863 80.9 db_names &lt;- c(&quot;fluDB&quot;, &quot;dengueDB&quot;, &quot;gapminderDB&quot;) tables &lt;- list() for (i in db_names){ tables[[paste(i)]] &lt;- dbReadTable(con, i) } 7.3 Joining dataset To join the Influenza and dengue dataset with the gapminder dataset there is one problem: tables$fluDB$year %&gt;% unique() tables$dengueDB$year %&gt;% unique() ## [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 ## [1] 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 The data of flu and dengue are from the year 2002 until 2015. while the data of gapminder is from: tables$gapminderDB$year %&gt;% min() tables$gapminderDB$year %&gt;% max() ## [1] 1960 ## [1] 2016 1960 till 2016. Also the datasets contain data about different countries: amount_factor(tables$fluDB$country) amount_factor(tables$dengueDB$country) amount_factor(tables$gapminderDB$country) ## [1] &quot;tables$fluDB$country has 29 levels.&quot; ## [1] &quot;tables$dengueDB$country has 10 levels.&quot; ## [1] &quot;tables$gapminderDB$country has 185 levels.&quot; levelF &lt;- tables$fluDB$country %&gt;% as.factor %&gt;% levels() levelD &lt;- tables$dengueDB$country %&gt;% as.factor %&gt;% levels() levelG &lt;- tables$gapminderDB$country %&gt;% as.factor %&gt;% levels() common(levelD, levelG) %&gt;% length() == length(levelD) #common countries with gapminder equal to total countries of dengue ## [1] TRUE All the countries in the dengue data set are all also in the gapminder dataset. However the influenza dataset contains countries that the gapminder dataset does not. common(levelF, levelG) %&gt;% length() == length(levelF) #common countries with gapminder equal to total countries of influenza ## [1] FALSE levelF[!levelF %in% common(levelF, levelG)] # the countries not in gapminder ## [1] &quot;New.Zealand&quot; &quot;South.Africa&quot; &quot;United.States&quot; If the tables are to be joined they need to have data about only the countries and years equal to the countries and years that the tables have in common. Or the tables need to be joined in a way that NA is introduced where necessary. #trimming the gapminder dataset so it contains the data of the same dates as the influenza and dengue dataset gapminder_02_15 &lt;- tables$gapminderDB %&gt;% filter(between(year, 2002, 2015)) dbWriteTable(con, &quot;gapminder_02_15DB&quot;, gapminder_02_15, overwrite = T) CREATE TABLE joined_gapfluden AS SELECT &quot;gapminder_02_15DB&quot;.*, &quot;fluDB&quot;.influenza_activity, &quot;dengueDB&quot;.dengue_activity FROM &quot;gapminder_02_15DB&quot; LEFT JOIN &quot;fluDB&quot; ON public.&quot;fluDB&quot;.year = public.&quot;gapminder_02_15DB&quot;.year AND public.&quot;fluDB&quot;.country = public.&quot;gapminder_02_15DB&quot;.country LEFT JOIN &quot;dengueDB&quot; ON public.&quot;dengueDB&quot;.year = public.&quot;gapminder_02_15DB&quot;.year AND public.&quot;dengueDB&quot;.country = public.&quot;gapminder_02_15DB&quot;.country joined_gapfluden &lt;- dbReadTable(conn = con, &quot;joined_gapfluden&quot;) dbDisconnect(con) The joined dataset contains data about 185 different countries, during the time period from 2002 until 2015. And also the influenza cases per country during this time. joined_gapfluden$country &lt;- joined_gapfluden$country %&gt;% as.factor() joined_gapfluden$year &lt;- joined_gapfluden$year %&gt;% as.factor() joined_gapfluden$continent &lt;- joined_gapfluden$continent %&gt;% as.factor() joined_gapfluden$region &lt;- joined_gapfluden$region %&gt;% as.factor() Table 3: The joinend table containing the gapminder dataset with influenza activity and dengue activity for the countries available. Where data for influenza or dengue activity was not available NA is introduced. country year infant_mortality life_expectancy fertility population gdp continent region influenza_activity dengue_activity Argentina 2002 17.1 74.3 2.38 37889443 242076212334 Americas South America 0 0.000 Australia 2002 5.0 80.3 1.76 19514385 442135393399 Oceania Australia and New Zealand 0 NA Austria 2002 4.4 78.8 1.39 8114698 196998621885 Europe Western Europe 0 NA Belgium 2002 4.4 78.2 1.68 10364613 237741668285 Europe Western Europe 0 NA Bolivia 2002 53.7 68.7 3.98 8653343 8751510220 Americas South America 0 0.101 Brazil 2002 24.3 71.4 2.26 181045592 670512665737 Americas South America 174 0.073 7.4 Visualizing data in 2009 a strain of influenza, the H1N1 influenza strain caused a pandemic known as the swine flu. The European countries who had the most influenza cases are: EU_flu &lt;- joined_gapfluden %&gt;% dplyr::filter(continent == &quot;Europe&quot;, influenza_activity &gt;= 10000) %&gt;% dplyr::select(country) %&gt;% unique() %&gt;% dplyr::pull() %&gt;% as.vector() %&gt;% print() ## [1] &quot;Austria&quot; &quot;Germany&quot; &quot;Romania&quot; &quot;Bulgaria&quot; &quot;Russia&quot; &quot;Ukraine&quot; &quot;Belgium&quot; &quot;France&quot; Figure 2: influenza cases per country. The total influenza cases per year are shown for the european countries that exeeded a year total of 10000 cases. Global cases peaked in 2009 at 586671. Some countries had more influenza cases relative to their population during the peak than others. Figure 4: Relative influenza activity in europe during 2009. Influenza activity is relative to the countries population. The swine flu was active during 2009. Its visable in the map that Austria and Bulgaria had a high influenza activity during that time. It is visible that Austria was had a relatively high influenza activity. Could this have effected life expectancy in Austria? Figure 3: The total influanza cases per year of Austria (red). And the life expectancy in Austria (black). The spike in influenza cases during 2009 has no or neglible effect on the life expenctancy in Austria. Refrences "],["rmdutil-package.html", "8 RMDutil package", " 8 RMDutil package i have made an R package called RMDutil. its nothing much accept a few custom functions that i often use. Two of which i feel can be useful to almost anyone writing RMD documents. The functions capture_package_name() and format_version(), which enable the user to format a typical code chunk that load package with their respective version numbers. The github can be found here "],["the-effect-of-vaccinations.html", "9 The effect of vaccinations 9.1 Trimming &amp; Joining dataframes 9.2 Visualization", " 9 The effect of vaccinations in January of 2020 the first covid-19 cases where confirmed in Wuhan, Hubei, China. The SARS-COV-19 virus then continued to spread across the world. By now we have all lived through the effects of the SARS-COV-19. However with vaccinations programs now gaining speed in America, European countries, most of Asia-Pacific and starting in Africa, the SARS-COV-19 Pandemic seems to be coming to an end. In this report i want to show the effect of vaccination programs, and highlight certain events and their effect on the spread of SARS-COV-19. library(tidyverse) library(readxl) library(utils) library(kableExtra) library(captioner) library(zoo) The European Centre for Disease Prevention and Control (ECDC) provides dataset for vaccination rates and covid-19 cases and associated deaths. # data_daily_cases &lt;- read.csv(&quot;https://opendata.ecdc.europa.eu/covid19/nationalcasedeath_eueea_daily_ei/csv&quot;, na.strings = &quot;&quot;, fileEncoding = &quot;UTF-8-BOM&quot;) # date of access: data_weekly_testing &lt;- utils::read.csv(&quot;https://opendata.ecdc.europa.eu/covid19/testing/csv&quot;, na.strings = &quot;&quot;, fileEncoding = &quot;UTF-8-BOM&quot;) # date of access: data_hospital &lt;- utils::read.csv(&quot;https://opendata.ecdc.europa.eu/covid19/hospitalicuadmissionrates/csv&quot;, na.strings = &quot;&quot;, fileEncoding = &quot;UTF-8-BOM&quot;) # date of access: data_vaccine &lt;- utils::read.csv(&quot;https://opendata.ecdc.europa.eu/covid19/vaccine_tracker/csv&quot;, na.strings = &quot;&quot;, fileEncoding = &quot;UTF-8-BOM&quot;) # date of access: write.csv(data_weekly_testing, &quot;data/ECDC_testing.csv&quot;) write.csv(data_hospital, &quot;data/ECDC_hospital.csv&quot;) write.csv(data_vaccine, &quot;data/EVDV_vaccine.csv&quot;) 9.1 Trimming &amp; Joining dataframes For the visualization its necessary to have a single dataframe containing all the variables needed for the plot. Im planning to join the needed columns of the dataframes by their matching date and country rows. All the dataframes have the date in a year week format (YYYY-Www). But the column names for date and country columns vary, and the hospital dataset only has country column values as whole names, while vaccine only has it as country codes. Luckily the testing dataset has both. Also the Hospital admission dataset has daily covid related occupancy and weekly new COVID-19 related patients admissioned per 100k population. Only the weekly hospitol and ICU admissions are needed for this report. The variables needed for the visualization are: year_week - the date in YYYY-Www format country - name of country region - country code new_cases - Number of new confirmed COVID-19 cases population - population of country value - Number of patients or new admissions per 100 000 population Indicator - If hospital occupancy is either: Daily hospital occupancy (number of COVID-19 patients in hospital on a given day) Daily ICU occupancy (number of COVID-19 patients in ICU on a given day) Weekly new hospital admissions per 100k (weekly rate of new admissions of COVID-19 patients per 100 000 population) Weekly new ICU new admissions of COVID-19 patients per 100k. FirstDose - Number of first dose vaccine administered to individuals during the reporting week. SecondDose - Number of second dose vaccine administered to individuals during the reporting week. Vaccine - Name of vaccine COM = Comirnaty  Pfizer/BioNTech MOD = mRNA-1273  Moderna CN = BBIBV-CorV  CNBG SIN = Coronavac  Sinovac SPU = Sputnik V - Gamaleya Research Institute AZ = AZD1222  AstraZeneca UNK = UNKNOWN TargetGroup - Target group for vaccination. data_weekly_testing &lt;- data_weekly_testing %&gt;% dplyr::filter(level == &quot;national&quot;) data_weekly_testing &lt;- data_weekly_testing %&gt;% dplyr::select(country, country_code, year_week, new_cases, population) data_hospital &lt;- data_hospital %&gt;% dplyr::select(country, year_week, value, indicator) data_hospital &lt;- data_hospital %&gt;% dplyr::filter(indicator == &quot;Weekly new ICU admissions per 100k&quot; | indicator == &quot;Weekly new hospital admissions per 100k&quot;) data_vaccine &lt;- data_vaccine %&gt;% dplyr::select(YearWeekISO, Region, FirstDose, SecondDose, Vaccine, TargetGroup) data_vaccine &lt;- data_vaccine %&gt;% dplyr::rename(year_week = YearWeekISO, country_code = Region) The three datasets contain different ranges of dates. If the datasets are to be joined the dataset with the largest date range needs te be selected. In this case thats the Hospital admissions dataset. by joining the other datasets to the hospital admissions dataset NA values will be implemented for variables where no data is available for that date. Table 1: The earliest and latest dates of each dataset. The hospital admissions dataset contains the longest date range. However it just doesnt contain the 2 latest weeks that the Vaccinations dataset does contain. dataset Earliest.date Latest.date Testing 2020-W01 2021-W33 Hospital admissions 2020-W01 2021-W33 Vaccinations 2020-W52 2021-W35 The vaccine dataset has its data for FirstDose and SecondDose split between different vaccines and targetgroups. This will cause duplicate datapoints in the new_cases and value rows if the dataset are joined. So the vaccine dataset is collapsed so that FirstDose and SecondDose represent the total vaccinations given that week, no matter which vaccination or in which targetgroup. sum_firstdose_before &lt;- data_vaccine$FirstDose %&gt;% sum() data_vaccine &lt;- data_vaccine %&gt;% dplyr::group_by(year_week, country_code) %&gt;% dplyr::transmute(&quot;year_week&quot; = year_week, &quot;country_code&quot; = country_code, &quot;FirstDose&quot; = sum(FirstDose, na.rm = T), &quot;SecondDose&quot; = sum(SecondDose, na.rm = T)) %&gt;% unique() There is no change in the sum of FirstDose, so the collapsing of the data went correctly. data_vaccine$FirstDose %&gt;% sum() == sum_firstdose_before ## [1] TRUE There seem to be duplicate rows in the testing dataset, when the datasets get joined this could cause duplicate datapoints. data_weekly_testing %&gt;% unique() %&gt;% nrow() == data_weekly_testing %&gt;% nrow() ## [1] TRUE data_weekly_testing &lt;- data_weekly_testing %&gt;% unique() data_covid &lt;- dplyr::left_join(data_hospital, data_weekly_testing, by = c(&quot;country&quot;, &quot;year_week&quot;) ) data_covid &lt;- dplyr::left_join(data_covid, data_vaccine, by = c(&quot;year_week&quot;, &quot;country_code&quot;)) There is no diffrence between the sum of new_cases between the orginal dataset and the joined dataset so the join is succesful. data_weekly_testing %&gt;% dplyr::filter(country == &quot;Netherlands&quot;) %&gt;% pull(new_cases) %&gt;% sum(na.rm = T) - data_covid %&gt;% dplyr::filter(country == &quot;Netherlands&quot;, indicator == &quot;Weekly new hospital admissions per 100k&quot;) %&gt;% pull(new_cases) %&gt;% sum(na.rm = T) ## [1] 0 9.2 Visualization Figure 1: Positive COVID-19 test (left Y-axis) and the cumaltive vaccine doses given (right Y-axis). In this plot it can be seen that during the ramp up of vaccine shots given (starting in march), positive COVID-19 cases went down. However it can also be seen that startig June the positive COVID-19 cases shot up, while the vaccinated population during this time was almost half the population. The effect of vaccines cant really be seen in this plot. That is not to say that there is no effect, the true effect of vaccination could only really be shown in a graph like this once group immunity start having effect. Which is why proper precautions are important before group immunity starts having effect. However the effect of vaccination is visible in the occupancy of hospitals and ICU. Figure 2: The effect of a vaccinated population on hospital and ICU admissions.it can be seen the the weekly hospital (x) and ICU addmissions (x) decrease while the vaccinated population increases. In this plot it is seen that the weekly admission rate of the hospitals and ICU goes down as vaccinations start. And during the third wave (July 2021) the COVID-19 related hospital and ICU admissions is less compared to the occupancy before vaccination started. #Show the three highest new case amounts. (new cases has duplicate value&#39;s because of dose column) cumsum_data_covid$new_cases %&gt;% sort() %&gt;% tail(n=6) max_cases &lt;- cumsum_data_covid %&gt;% filter(Dose == &quot;FirstDose&quot;, new_cases == 80594 | new_cases == 76154 | new_cases == 70155) %&gt;% dplyr::select(year_week, new_cases, hospital, ICU) ## [1] 70132 70132 76154 76154 80594 80594 Table 2: Overview of the three highest case counts over the period of January 2020 untill July 2021. year_week new_cases hospital ICU 2020-W51 76154 1735 288 2020-W52 80594 1981 293 The three highest new case counts. Two from the peak during December 2020, and one from the peak during July 2021. The difference in hospital and ICU admissions is clear and could be attributed to the vaccinated population. "],["refrences.html", "10 Refrences", " 10 Refrences "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
